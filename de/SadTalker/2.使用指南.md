# SadTalker Bedienungsanleitung

SadTalker generiert realistische Talking-Head-Videos aus einem einzelnen Bild und Audio-Input mittels Deep Learning.

## Schnellstart

### Basisbefehl

```bash
python inference.py \
  --source_image examples/source_image.jpg \
  --driven_audio examples/driven_audio.wav \
  --result_dir ./results
```

### Video mit Verbesserung generieren

```bash
python inference.py \
  --source_image examples/source_image.jpg \
  --driven_audio examples/driven_audio.wav \
  --result_dir ./results \
  --preprocess full \
  --enhancer gfpgan
```

## Kommandozeilenparameter

### Eingabeparameter

| Parameter | Beschreibung | Standardwert |
|-----------|--------------|---------------|
| `--source_image` | Pfad zum Eingabebild (Porträt) | Erforderlich |
| `--driven_audio` | Pfad zur Audiodatei (WAV/MP3) | Erforderlich |
| `--result_dir` | Ausgabeverzeichnis | ./results |

### Verarbeitungsparameter

| Parameter | Beschreibung | Standardwert |
|-----------|--------------|---------------|
| `--preprocess` | Bildvorverarbeitung: crop, resize, full | full |
| `--size` | Bildgröße: 256, 512 | 256 |
| `--pose_style` | Pose-Stil 0-45 | 0 |
| `--expression_scale` | Ausdrucksintensität 0.5-1.5 | 1.0 |

### Verbesserungsparameter

| Parameter | Beschreibung | Standardwert |
|-----------|--------------|---------------|
| `--enhancer` | Gesichtsverbesserer: gfpgan, RestoreFormer, CodeFormer | Keiner |
| `--enhancer_background` | Hintergrund verbessern | False |

### Leistungsparameter

| Parameter | Beschreibung | Standardwert |
|-----------|--------------|---------------|
| `--batch_size` | Batch-Größe für Verarbeitung | 1 |
| `--fps` | Ausgabevideo FPS | 25 |
| `--faceid` | Gesichtserkennung beibehalten | False |

### Sonstige Parameter

| Parameter | Beschreibung | Standardwert |
|-----------|--------------|---------------|
| `--help` | Alle Optionen anzeigen | - |

## Verwendungsbeispiele

### Beispiel 1: Basisgenerierung

```bash
python inference.py \
  --source_image examples/source_image.jpg \
  --driven_audio examples/driven_audio.wav \
  --result_dir results/basic
```

### Beispiel 2: Mit GFPGAN-Verbesserung

```bash
python inference.py \
  --source_image examples/source_image.jpg \
  --driven_audio examples/driven_audio.wav \
  --result_dir results/enhanced \
  --enhancer gfpgan
```

### Beispiel 3: Benutzerdefinierter Pose-Stil

```bash
python inference.py \
  --source_image examples/source_image.jpg \
  --driven_audio examples/driven_audio.wav \
  --result_dir results/pose5 \
  --pose_style 5
```

### Beispiel 4: Höhere Auflösung

```bash
python inference.py \
  --source_image examples/source_image.jpg \
  --driven_audio examples/driven_audio.wav \
  --result_dir results/hd \
  --size 512 \
  --enhancer gfpgan
```

## Python API

### Grundlegende Verwendung

```python
from inference import SadTalker

# SadTalker initialisieren
sadtalker = SadTalker()

# Video generieren
video_path = sadtalker.generate(
    source_image="image.jpg",
    driven_audio="audio.wav",
    preprocess="full",
    enhancer="gfpgan"
)

print(f"Video gespeichert unter: {video_path}")
```

### Erweiterte Verwendung

```python
from inference import SadTalker
import torch

# Mit benutzerdefinierten Einstellungen initialisieren
sadtalker = SadTalker(
    checkpoint_path="checkpoints/SadTalker.pth",
    config_path="config/SadTalker.yaml",
    device="cuda" if torch.cuda.is_available() else "cpu"
)

# Mit erweiterten Optionen generieren
video_path = sadtalker.generate(
    source_image="image.jpg",
    driven_audio="audio.wav",
    preprocess="full",
    pose_style=10,
    expression_scale=1.2,
    enhancer="gfpgan",
    batch_size=1,
    output_video="output.mp4"
)
```

### Stapelverarbeitung

```python
from inference import SadTalker
import os

sadtalker = SadTalker()

# Mehrere Bilder mit derselben Audiodatei verarbeiten
audio_file = "audio.wav"
image_dir = "images/"

for image_file in os.listdir(image_dir):
    if image_file.endswith(('.jpg', '.png')):
        output_path = f"results/{image_file}"
        sadtalker.generate(
            source_image=os.path.join(image_dir, image_file),
            driven_audio=audio_file,
            preprocess="full"
        )
```

## Webinterface

### Web-Demo ausführen

```bash
python app.py
```

Dann im Browser http://localhost:8888 öffnen

### Webinterface-Funktionen

1. **Bild-Upload**: Porträtbild per Drag & Drop oder Auswahl hochladen
2. **Audio-Upload**: Audiodatei auswählen (WAV/MP3)
3. **Vorschau**: Quellbild und Audio anzeigen
4. **Parameter**: Pose, Ausdruck, Verbesserer anpassen
5. **Generieren**: Video mit einem Klick erstellen
6. **Herunterladen**: Generiertes Video speichern

### Webinterface-Steuerung

| Steuerung | Beschreibung |
|-----------|--------------|
| Source Image | Porträtfoto hochladen |
| Driven Audio | Sprach-Audio hochladen |
| Preprocess | Vorverarbeitungsmodus wählen |
| Pose Style | Animationsstil für Pose auswählen |
| Enhancer | Gesichtsqualitätsverbesserer auswählen |
| Generate Button | Videogenerierung starten |

## Gesichtsverbesserer

### Verfügbare Verbesserer

| Verbesserer | Qualität | Geschwindigkeit | VRAM |
|-------------|----------|-----------------|------|
| Keiner | Basis | Schnell | Niedrig |
| gfpgan | Gut | Mittel | Mittel |
| RestoreFormer | Sehr gut | Langsam | Hoch |
| CodeFormer | Sehr gut | Langsam | Hoch |

### Empfehlung

- **Niedriger VRAM**: Kein Verbesserer oder gfpgan
- **Ausgewogen**: gfpgan
- **Beste Qualität**: RestoreFormer oder CodeFormer

## Vorverarbeitungsmodi

| Modus | Beschreibung | Anwendungsfall |
|-------|--------------|----------------|
| crop | Gesicht zentrieren | Schnelle Verarbeitung |
| resize | Auf Zielgröße ändern | Einfache Hintergründe |
| full | Vollständige Pipeline | Beste Ergebnisse |

## Eingabeanforderungen

### Bilanforderungen

- Format: JPG, PNG
- Größe: 512x512 oder größer empfohlen
- Gesicht: Frontal, klare Merkmale
- Hintergrund: Einfach bevorzugt

### Audioanforderungen

- Format: WAV, MP3
- Dauer: 1-60 Sekunden
- Abtastrate: 16000-48000 Hz
- Sprache: Klares Audio mit minimalem Rauschen

## Ausgabe

### Videoformat

- Format: MP4
- Codec: H.264
- Auflösung: 256x256 oder 512x512
- FPS: 25
- Bitrate: 2-5 Mbps

### Ausgabeverzeichnis

Ergebnisse werden im angegebenen Ergebnisverzeichnis gespeichert:
```
results/
├── video.mp4          # Generiertes Video
├── video_idx.mp4     # Mit Index (falls mehrere)
└── (Temporäre Dateien)
```

## Fehlerbehebung

### Video zu dunkel/hell

**Ursache:** Beleuchtungsunterschied zwischen Training und Eingabe

**Lösungen:**
- expression_scale-Parameter anpassen (0.8-1.2)
- Besseres Quellbild verwenden
- Verschiedenen Verbesserer ausprobieren

### Lippen nicht synchron

**Ursache:** Audioqualität oder Ausrichtungsprobleme

**Lösungen:**
- Hochwertiges Audio verwenden
- Sicherstellen, dass Audio klare Sprache enthält
- Audio-Video-Synchronisation prüfen
- Audio auf Videolänge zuschneiden

### Gesichtsverzerrung

**Ursache:** Niedrige Qualität oder ungewöhnliche Eingabe

**Lösungen:**
- Bild mit höherer Auflösung verwenden (512)
- Verschiedenen Vorverarbeitungsmodus versuchen
- Gesichtsverbesserer verwenden
- Sicherstellen, dass Gesicht klar sichtbar ist

### Langsame Verarbeitung

**Lösungen:**
- Batch-Größe reduzieren
- Niedrigere Auflösung (--size 256)
- Verbesserer deaktivieren
- Schnellere Vorverarbeitung verwenden (crop)

### GPU Out of Memory

**Lösungen:**
- batch_size auf 1 reduzieren
- Kleinere Bildgröße verwenden
- Andere GPU-Anwendungen schließen
- CPU-Offloading aktivieren

## Leistungstipps

### Schnellere Verarbeitung

1. `--preprocess crop` verwenden
2. `--size 256` einstellen
3. Verbesserer deaktivieren
4. Batch-Größe reduzieren

### Bessere Qualität

1. `--size 512` verwenden
2. `--enhancer gfpgan` aktivieren
3. `--preprocess full` verwenden
4. `--expression_scale` anpassen

### VRAM sparen

1. Eins nach dem anderen verarbeiten
2. Kleinere Modelle verwenden
3. Unnötige Funktionen deaktivieren

## Integrationsbeispiele

### Python Flask API

```python
from flask import Flask, request, send_file
from inference import SadTalker
import tempfile

app = Flask(__name__)
sadtalker = SadTalker()

@app.route('/generate', methods=['POST'])
def generate():
    image = request.files['image']
    audio = request.files['audio']
    
    with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as img:
        image.save(img.name)
    
    with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as aud:
        audio.save(aud.name)
    
    video_path = sadtalker.generate(
        source_image=img.name,
        driven_audio=aud.name,
        preprocess="full"
    )
    
    return send_file(video_path, mimetype='video/mp4')
```

## Verwandte Links

- [GitHub](https://github.com/OpenTalker/SadTalker)
- [HuggingFace](https://huggingface.co/spaces/fffilo/SadTalker)
- [Paper](https://arxiv.org/abs/2303.17550)
- [Modelle](https://github.com/OpenTalker/SadTalker/releases)
