# V-Express Frequently Asked Questions

## General Questions

### What is V-Express?
V-Express is an AI model that generates talking head videos from a single portrait image and audio input. It creates realistic facial animations with accurate lip synchronization.

### Is V-Express free to use?
V-Express is open-source for research purposes. Check the LICENSE for commercial usage terms.

### What makes V-Express different from other talking face generators?
V-Express uses:
- Progressive conditional diffusion decoding
- Weakness-aware VQ-Module
- Multi-level conditioning
- Better handling of weak expressions

## Installation Questions

### What are the requirements?
```bash
# Python environment
conda create -n vexpress python=3.8
conda activate vexpress

# Install dependencies
pip install torch torchvision
pip install opencv-python
pip install librosa
pip install -r requirements.txt
```

### How do I download the models?
```bash
bash scripts/download_models.sh
```

Or manually download from the releases page.

### Do I need a powerful GPU?
Yes, a GPU with 8GB+ VRAM is recommended. Higher VRAM allows for better quality and faster processing.

## Usage Questions

### How do I generate a talking video?
```bash
python inference.py \
    --reference_image path/to/image.jpg \
    --audio path/to/audio.wav \
    --output output.mp4
```

### What image formats are supported?
- JPG/JPEG
- PNG
- BMP
- WebP

### What audio formats are supported?
- WAV (recommended)
- MP3
- FLAC

### How long does it take to generate a video?
Processing time depends on:
- Video length (audio duration)
- GPU performance
- Output resolution
- Quality settings

Typical: 1-5 minutes per second of video on GPU.

### Can I use a video reference?
Yes, provide a video reference for motion patterns:
```bash
python inference.py \
    --reference_image path/to/image.jpg \
    --audio path/to/audio.wav \
    --reference_video path/to/video.mp4 \
    --output output.mp4
```

## Troubleshooting

### Poor lip synchronization
- Use high-quality audio without background noise
- Ensure clear speech in the audio
- Try adjusting the audio feature extraction parameters

### Identity distortion
- Use a high-resolution, front-facing image
- Avoid heavily filtered or edited photos
- Ensure good lighting in the source image

### Artifacts in output
- Reduce output resolution
- Check for proper model installation
- Update GPU drivers

### Out of memory errors
- Reduce batch size
- Lower output resolution
- Use gradient checkpointing
- Close other GPU applications

## Advanced Questions

### How do I control expression strength?
```bash
python inference.py \
    --reference_image path/to/image.jpg \
    --audio path/to/audio.wav \
    --expression_strength 0.7 \
    --output output.mp4
```

### Can I generate with specific poses?
Use pose control parameters or provide a reference video with desired poses.

### What is the Weakness-aware VQ-Module?
It's a component that handles challenging expressions by learning from weak signals in the training data, improving overall expression quality.

## Best Practices

1. Use high-resolution, clear portrait images
2. Use clean audio without background noise
3. Provide video reference for better motion
4. Adjust parameters for specific use cases
5. Use consistent lighting in source images

## Ethical Considerations

- Obtain consent before using others' images
- Be transparent about AI-generated content
- Follow applicable laws and regulations
- Use responsibly to prevent misuse

## Resources

- GitHub Repository: https://github.com/TencentARC/V-Express
- Paper: V-Express paper
- Community: GitHub Issues
