# DragonTalker User Manual

> Detailed instructions for DragonTalker features, usage methods and development guide
> 
> Updated: February 2026

---

## 1. Feature Overview

DragonTalker is a deep learning-based talking head generation system. Its core function is to combine a static human image with an audio file to generate dynamic videos with talking facial movements. This technology uses advanced 3D facial reconstruction and image synthesis algorithms to preserve the original person's facial features while achieving natural lip synchronization and expression changes.

### 1.1 Supported Application Scenarios

- **Virtual Anchors**: Configure virtual avatars with realistic voice and expressions
- **Digital Human Customer Service**: Generate anthropomorphic interaction videos
- **Education and Training**: Present course content in dynamic video form
- **Film and TV Dubbing**: Quickly generate dubbing animations for characters
- **Historical Figure Recreation**: Synthesize images from historical photos and audio (with authorization)

### 1.2 Technical Features

- Highly realistic: Generated facial expressions are natural and smooth, difficult to distinguish from real
- Fast generation: Single image and audio can be processed within minutes
- Diverse poses: Supports multiple head poses and facial expression changes
- Optional quality: Choose different quality enhancement levels based on needs

---

## 2. Quick Start

### 2.1 Basic Usage

After installation, the simplest way to use it is command-line inference:

```bash
# Activate virtual environment
source venv/bin/activate  # Linux/macOS
# or
venv\Scripts\activate     # Windows

# Run inference script
python inference.py \
  --source_image examples/source_image.jpg \
  --driven_audio examples/driven_audio.wav \
  --result_dir ./results
```

### 2.2 Parameter Description

| Parameter | Short | Description | Default |
|-----------|-------|-------------|---------|
| --source_image | -i | Source human image path | Required |
| --driven_audio | -a | Driving audio path | Required |
| --result_dir | -o | Output video directory | ./results |
| --preprocess | -p | Image preprocessing mode | full |
| --enhancer | -e | Face enhancement algorithm | gfpgan |
| --outputfps | -f | Output video frame rate | 25 |
| --faceid_weight | -w | Identity preservation weight | 0.5 |

### 2.3 Preprocessing Modes

| Mode | Description | Use Case |
|------|-------------|-----------|
| full | Full preprocessing including face alignment and lighting adjustment | General scenarios |
| crop | Only crop face region | Images with large face proportion |
| resize | Only adjust resolution | Quick preview |

### 2.4 Enhancement Algorithms

| Algorithm | Effect | Speed | VRAM |
|-----------|--------|-------|------|
| gfpgan | Good overall restoration | Fast | 2GB |
| restoreformer | Fine facial details | Medium | 4GB |
| codeformer | Balanced quality and speed | Medium | 3GB |
| none | No enhancement | Fastest | 0 |

---

## 3. Input/Output Specifications

### 3.1 Source Image Requirements

**File formats**: JPG, PNG, BMP and other common formats. PNG recommended for best quality

**Resolution requirements**:
- Minimum: 256×256 pixels
- Recommended: 512×512 pixels or higher
- Optimal: 1024×1024 pixels

**Image content**:
- Face should occupy the main area of the image
- Frontal face without hat recommended
- Clear facial features, no large-area occlusion
- Even lighting, avoid extreme backlight or side light
- Natural expressions, neutral expressions recommended

**Image examples**:

```
Correct examples:
├── front_photo.jpg       # Frontal face, no hat, natural expression
├── portrait.png        # Even lighting, high quality
└── studio_photo.png     # Simple background

Incorrect examples:
├── side_profile.jpg     # Excessive side angle
├── occluded_photo.jpg  # Sunglasses, masks, etc.
├── blurry_photo.jpg    # Motion blur or out of focus
└── group_photo.jpg     # Multiple faces
```

### 3.2 Driving Audio Requirements

**, MP3, M4A, AAC and other mainstream audio formats

**File formats**: WAVAudio characteristics**:
- Sample rate: 16kHz or 44.1kHz recommended
- Duration: 1 to 60 seconds is ideal
- Content: Clear human voice
- Encoding: PCM or MP3 encoding recommended

**Audio preprocessing**:

If audio does not meet requirements, use ffmpeg for preprocessing:

```bash
# Convert format
ffmpeg -i input.wav -ar 16000 output.wav

# Trim duration
ffmpeg -i input.wav -ss 0 -t 30 output.wav

# Noise reduction (optional)
ffmpeg -i input.wav -af "anoisesrc=p=-30" output.wav
```

### 3.3 Output Video Specifications

| Parameter | Optional Values | Default |
|----------|-----------------|----------|
| Video format | MP4 (H.264) | MP4 |
| Resolution | 256×256, 512×512 | 256×256 |
| Frame rate | 25, 30, 60 | 25 |
| Video bitrate | 1M, 2M, 4M | 2M |

---

## 4. Web Interface Usage

DragonTalker provides a web graphical interface for non-technical users.

### 4.1 Start Web Service

```bash
# Start web service, default port 7860
python app.py

# Specify port
python app.py --port 8080

# Allow LAN access
python app.py --share true
```

### 4.2 Interface Functions

Open browser and visit `http://localhost:7860`. The interface includes:

1. **Image upload area**: Drag or click to upload source image
2. **Audio upload area**: Upload driving audio file
3. **Parameter settings area**: Adjust generation parameters
4. **Preview area**: View generation results
5. **Download area**: Save generated videos

### 4.3 Batch Processing

Web interface supports batch uploading multiple audio segments:

```bash
# Batch processing
python batch_inference.py \
  --source_image examples/source.jpg \
  --audio_dir ./audios \
  --output_dir ./batch_results
```

---

## 5. Python API Usage

DragonTalker provides complete Python API for programmatic calls and secondary development.

### 5.1 Basic Call Flow

```python
from dragon_talker import DragonTalker

# Initialize model
model = DragonTalker(
    device='cuda',           # Use GPU acceleration
    faceid_weight=0.5,     # Identity preservation weight
    enhancer='gfpgan'         # Face enhancement algorithm
)

# Load image and audio
model.load_source('source.jpg')
model.load_audio('audio.wav')

# Generate video
result_path = model.generate(
    output_path='result.mp4',
    preprocess='full',
    fps=25
)

print(f"Video saved to: {result_path}")
```

### 5.2 Advanced Usage

```python
from dragon_talker import DragonTalker, FaceDetector, AudioProcessor

# Custom face detector
face_detector = FaceDetector(
    model='retinaface',
    device='cuda'
)

# Custom audio processor
audio_processor = AudioProcessor(
    sample_rate=16000,
    normalize=True
)

# Combine usage
model = DragonTalker(
    face_detector=face_detector,
    audio_processor=audio_processor
)
```

### 5.3 Return Value Handling

```python
# Generation result includes detailed information
result = model.generate(return_dict=True)

print(f"Video path: {result['video_path']}")
print(f"Processing time: {result['processing_time']:.2f}s")
print(f"Total frames: {result['total_frames']}")
print(f"Average FPS: {result['avg_fps']:.1f}")
```

---

## 6. Custom Training

To train custom models, refer to the following steps:

### 6.1 Data Preparation

Training data directory structure:

```
dataset/
├── train/
│   ├── videos/
│   │   ├── person001/
│   │   │   ├── video001.mp4
│   │   │   └── video002.mp4
│   │   └── person002/
│   └── audios/
│       ├── person001/
│       └── person002/
└── val/
    └── (same structure as above)
```

### 6.2 Start Training

```bash
# Single GPU training
python train.py --config configs/train_base.yaml

# Multi-GPU training
python -m torch.distributed.launch \
  --nproc_per_node=4 \
  train.py \
  --config configs/train_base.yaml
```

### 6.3 Training Parameters

| Parameter | Description | Common Values |
|-----------|-------------|----------------|
| --batch_size | Batch size | 8-32 |
| --lr | Learning rate | 1e-4 |
| --epochs | Training epochs | 100-500 |
| --log_interval | Log output interval | 100 |

---

## 7. FAQ

### 7.1 Video Quality

**Problem: Generated face is blurry**

Solution: Enable face enhancement with gfpgan or restoreformer algorithm

```bash
python inference.py --enhancer gfpgan ...
```

---

**Problem: Lips not synchronized with audio**

Possible causes: Poor audio quality or too fast speech

Solutions:
1. Preprocess audio, increase sample rate
2. Adjust audio preprocessing parameters
3. Use clearer audio source

---

**Problem: Face appears distorted**

Possible causes: Source image angle too large or exaggerated expression

Solution: Use frontal face photo without hat as source image

---

### 7.2 Performance

**Problem: Insufficient GPU VRAM**

Solutions:
1. Reduce output resolution: `--output_size 256`
2. Disable enhancement: `--enhancer none`
3. Use CPU mode: `--device cpu` (slower)

---

**Problem: Processing too slow**

Optimization suggestions:
1. Use GPU acceleration
2. Keep Web service pre-started and maintain model loaded state
3. Use longer warm-up time for batch processing

---

### 7.3 Error Handling

**Problem: "No face detected in source image"**

Solution:
1. Check if image contains clear face
2. Confirm image format is correct
3. Try other face images

---

**Problem: "Audio processing failed"**

Solution:
1. Confirm audio file format is correct
2. Use ffmpeg to convert audio format
3. Check if audio contains valid speech content

---

## 8. Performance Optimization

### 8.1 Memory Optimization

```python
# Use mixed precision inference
model = DragonTalker(
    precision='fp16',  # Half precision mode, half VRAM usage
    tile_size=512     # Tile processing, no VRAM overflow for large images
)
```

### 8.2 Batch Processing Optimization

```python
# Batch load audio
from dragon_talker import AudioBatchLoader

loader = AudioBatchLoader(
    audio_dir='./audios',
    batch_size=10,
    preprocess=True
)

for batch in loader:
    results = model.generate_batch(batch)
```

### 8.3 Distributed Processing

For large-scale video generation needs, use multi-machine distributed processing:

```bash
# Start master node
python distribute.py --master --port 22222

# Start worker nodes
python distribute.py --worker --master-addr 192.168.1.100:22222
```

---

## 9. API Reference

### 9.1 DragonTalker Class

```python
class DragonTalker:
    def __init__(self, device='cuda', faceid_weight=0.5, enhancer='gfpgan'):
        """Initialize model
        
        Parameters:
            device: Computing device, 'cuda' or 'cpu'
            faceid_weight: Identity preservation weight, between 0-1
            enhancer: Face enhancement algorithm
        """
        
    def load_source(self, image_path):
        """Load source image"""
        
    def load_audio(self, audio_path):
        """Load driving audio"""
        
    def generate(self, output_path=None, preprocess='full', fps=25, return_dict=False):
        """Generate talking video
        
        Returns:
            str or dict: Video path or detailed information dictionary
        """
```

### 9.2 Configuration Reference

For detailed configuration parameters, refer to `config/default_config.yaml`.

---

## 10. Related Resources

- Official Demo: https://huggingface.co/spaces/dragon-talker
- Model Download: Automatically downloaded from HuggingFace Hub
- Technical Paper: See `docs/paper.md`
- Changelog: See `CHANGELOG.md`
