# LivePortrait FAQ

## General Questions

### What is LivePortrait?
LivePortrait is an AI technology that animates static portrait images using deep learning. It can make portraits move and express emotions based on driving inputs.

### What can I use LivePortrait for?
- Animating static photos
- Creating talking avatars
- Expression transfer between subjects
- Video conferencing avatars
- Entertainment content creation

### Is LivePortrait free to use?
Please check the GitHub repository for licensing information.

## Technical Questions

### What are the system requirements?
- GPU with CUDA support (recommended)
- Sufficient VRAM for model inference
- Standard deep learning environment (PyTorch, etc.)

### What input formats are supported?
- Static images (JPG, PNG)
- Driving videos
- Audio input (for audio-driven animation)

### How long does processing take?
Processing time depends on:
- Hardware capabilities
- Input resolution
- Model configuration

## Usage Questions

### How do I get started?
1. Clone the GitHub repository
2. Install dependencies
3. Prepare your input images/videos
4. Run the inference script

### Can I use my own models?
Yes, the framework supports custom model training and fine-tuning.

### Is commercial use allowed?
Please refer to the project's license terms.

## Resources

- GitHub: https://github.com/KwaiVGI/LivePortrait
- Documentation in repository
