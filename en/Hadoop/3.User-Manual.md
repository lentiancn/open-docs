# Hadoop User Manual

---

## HDFS Operations

```bash
# List files
hdfs dfs -ls /

# Create directory
hdfs dfs -mkdir -p /user/data

# Upload file
hdfs dfs -put local.txt /user/data/

# Download file
hdfs dfs -get /user/data/output.txt

# Delete file
hdfs dfs -rm /user/data/file.txt
```

## MapReduce WordCount Example

```java
public static class Mapper extends Mapper<Object, Text, Text, IntWritable> {
    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();
    
    public void map(Object key, Text value, Context context) {
        String[] words = value.toString().split("\\s+");
        for (String w : words) {
            word.set(w);
            context.write(word, one);
        }
    }
}

public static class Reducer extends Reducer<Text, IntWritable, Text, IntWritable> {
    public void reduce(Text key, Iterable<IntWritable> values, Context context) {
        int sum = 0;
        for (IntWritable val : values) {
            sum += val.get();
        }
        context.write(key, new IntWritable(sum));
    }
}
```

## YARN Operations

```bash
hadoop jar wordcount.jar WordCount /input /output
yarn application -list
yarn logs -applicationId <application_id>
```

## Performance Optimization

1. Set appropriate block size
2. Compress data to reduce I/O
3. Optimize Map/Reduce count
4. Use combiner to reduce data transfer
