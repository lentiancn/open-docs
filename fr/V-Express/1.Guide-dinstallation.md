# V-Express Guide d'installation

V-Express est une méthode de génération de vidéos de portrait développée par Tencent AI Lab. Elle génère des vidéos de tête parlante sous le contrôle d'une image de référence, d'un audio et de séquences V-Kps.

## Vue d'ensemble des versions

| Version | Statut | Description |
|---------|--------|-------------|
| V-Express 1.x | Actuelle | Version principale |

## Configuration requise

### Matériel

| Composant | Minimum | Recommandé |
|-----------|---------|-------------|
| GPU | NVIDIA 16GB VRAM | NVIDIA 24GB+ VRAM |
| RAM | 16GB | 32GB+ |
| Stockage | 50GB | 100GB+ |
| CUDA | 11.8+ | 12.1+ |

### Logiciel

| Logiciel | Version | Notes |
|----------|---------|-------|
| Python | 3.8 - 3.10 | Recommandé 3.9 |
| CUDA | 11.8+ | Accélération GPU requise |
| cuDNN | 8.5+ | Requis pour CUDA |
| ffmpeg | Dernière | Traitement vidéo requis |

## Installation Linux/Ubuntu

### Étape 1 : Installer les dépendances système

```bash
# Mettre à jour le système
sudo apt update && sudo apt upgrade -y

# Installer Python et outils de développement
sudo apt install python3.9 python3-pip python3-venv git wget curl

# Installer les bibliothèques système
sudo apt install libgl1-mesa-glx libglib2.0-0 libsm6 libxext6 libxrender-dev libgomp1

# Installer ffmpeg
sudo apt install ffmpeg

# Installer git-lfs
sudo apt install git-lfs
```

### Étape 2 : Cloner le projet

```bash
git clone https://github.com/tencent-ailab/V-Express.git
cd V-Express
```

### Étape 3 : Créer un environnement virtuel

```bash
python3.9 -m venv venv
source venv/bin/activate

# Vérifier la version de Python
python --version
```

### Étape 4 : Installer PyTorch

```bash
# Version CUDA 11.8
pip install torch==2.0.1 torchvision==0.15.2 --index-url https://download.pytorch.org/whl/cu118

# Vérifier l'installation
python -c "import torch; print(torch.cuda.is_available())"
```

### Étape 5 : Installer les dépendances

```bash
pip install -r requirements.txt
```

### Étape 6 : Télécharger les modèles

```bash
# Installer git-lfs
git lfs install

# Cloner les modèles depuis HuggingFace
git clone https://huggingface.co/tk93/V-Express

# Déplacer les modèles à l'emplacement correct
mv V-Express/model_ckpts model_ckpts
mv V-Express/*.bin model_ckpts/v-express
```

Alternativement, télécharger les modèles individuellement :

```bash
# Créer le répertoire des modèles
mkdir -p model_ckpts

# Télécharger les modèles requis :
# - stabilityai/sd-vae-ft-mse
# - run wayml/stable-diffusion-v1-5 (config unet seulement)
# - facebook/wav2vec2-base-960h
# - insightface/buffalo_l
```

## Installation Windows

### Étape 1 : Installer Python

1. Télécharger Python 3.9 depuis python.org
2. Cocher "Add Python to PATH" pendant l'installation
3. Vérifier : `python --version`

### Étape 2 : Installer CUDA

1. Télécharger CUDA 11.8 depuis le site NVIDIA
2. Installer avec les paramètres par défaut
3. Télécharger et installer cuDNN 8.9

### Étape 3 : Cloner et configurer

```powershell
git clone https://github.com/tencent-ailab/V-Express.git
cd V-Express

python -m venv venv
venv\Scripts\activate
```

### Étape 4 : Installer les dépendances

```powershell
pip install torch==2.0.1 torchvision==0.15.2 --index-url https://download.pytorch.org/whl/cu118

pip install -r requirements.txt
```

### Étape 5 : Télécharger les modèles

Télécharger les modèles depuis HuggingFace et les placer dans le répertoire `model_ckpts/`.

## Installation macOS

### Étape 1 : Installer les dépendances

```bash
# Installer Homebrew (si non installé)
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# Installer Python et ffmpeg
brew install python@3.9 ffmpeg git git-lfs
```

### Étape 2 : Configurer

```bash
git clone https://github.com/tencent-ailab/V-Express.git
cd V-Express

python3.9 -m venv venv
source venv/bin/activate

# Installer PyTorch (CPU seulement sur macOS)
pip install torch==2.0.1 torchvision==0.15.2

pip install -r requirements.txt
```

Note : macOS supporte uniquement le mode CPU, l'accélération GPU n'est pas disponible.

## Vérifier l'installation

```bash
# Tester la fonctionnalité de base
python inference.py --help
```

## Problèmes courants

### Mémoire CUDA insuffisante

**Solutions :**
- Réduire les étapes d'inférence : `--num_inference_steps 15`
- Activer l'économie de mémoire : `--save_gpu_memory`
- Utiliser une résolution plus basse

### Bibliothèques manquantes (Windows)

**Solutions :**
- Installer Visual C++ Redistributable 2015-2022
- Ajouter CUDA au PATH système

### Échec du téléchargement des modèles

**Solutions :**
- Utiliser un VPN ou un proxy
- Télécharger manuellement depuis HuggingFace
- Vérifier la connexion réseau

### ffmpeg introuvable

**Solutions :**
- Linux : `sudo apt install ffmpeg`
- Windows : Ajouter ffmpeg au PATH
- macOS : `brew install ffmpeg`

## Notes importantes

1. **Recadrage du visage** : Lorsque la vidéo cible n'est pas la même personne que l'image de référence, le recadrage du visage est important. Choisissez une vidéo cible avec une pose plus similaire au visage de référence pour de meilleurs résultats.

2. **Support linguistique** : Le modèle fonctionne mieux en anglais ; les autres langues n'ont pas été testées en détail.

3. **Qualité d'image** : Utilisez des images de visage carrées nettes avec une résolution d'au moins 512x512.

## Liens connexes

- [Dépôt GitHub](https://github.com/tencent-ailab/V-Express)
- [Modèles HuggingFace](https://huggingface.co/tk93/V-Express)
- [Article](https://arxiv.org/abs/2406.02511)
- [Page du projet](https://tenvence.github.io/p/v-express/)
