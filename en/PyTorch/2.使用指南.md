# PyTorch Usage Guide

Comprehensive guide for using PyTorch effectively.

---

## Table of Contents

- [Basic Concepts](#basic-concepts)
- [Tensors](#tensors)
- [Neural Networks](#neural-networks)
- [Training](#training)
- [Data Handling](#data-handling)
- [GPU Support](#gpu-support)
- [Model Saving/Loading](#model-savingloading)
- [Transfer Learning](#transfer-learning)
- [Best Practices](#best-practices)

---

## Basic Concepts

### What is PyTorch?

PyTorch is an open-source machine learning framework developed by Facebook. It provides a flexible platform for building deep learning models.

### Key Components

| Component | Description |
|-----------|-------------|
| **torch** | Core tensor library |
| **torch.nn** | Neural network modules |
| **torch.optim** | Optimization algorithms |
| **torch.utils.data** | Data loading utilities |
| **torchvision** | Computer vision utilities |

---

## Tensors

### Creating Tensors

```python
import torch

# From Python list
tensor = torch.tensor([1, 2, 3])

# Zeros and ones
zeros = torch.zeros(3, 3)
ones = torch.ones(2, 4)

# Random
rand = torch.randn(3, 3)
randn = torch.rand(2, 2)

# From NumPy
import numpy as np
arr = np.array([1, 2, 3])
tensor = torch.from_numpy(arr)

# Tensor on GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
gpu_tensor = torch.zeros(3, 3, device=device)
```

### Tensor Operations

```python
import torch

a = torch.tensor([[1, 2], [3, 4]])
b = torch.tensor([[5, 6], [7, 8]])

# Addition
c = torch.add(a, b)
c = a + b

# Multiplication
c = torch.mul(a, b)  # Element-wise
c = torch.matmul(a, b)  # Matrix
c = a @ b

# Other operations
c = torch.sum(a)
c = torch.mean(a)
c = a.reshape(4, 1)
c = a.view(4, 1)
c = a.T  # Transpose
```

### Tensor Attributes

```python
import torch

tensor = torch.randn(3, 4)

print(f"Shape: {tensor.shape}")
print(f"dtype: {tensor.dtype}")
print(f"device: {tensor.device}")
print(f"requires_grad: {tensor.requires_grad}")

# Change properties
tensor = tensor.to(torch.float32)
tensor = tensor.to('cuda')
```

---

## Neural Networks

### Using torch.nn

```python
import torch
import torch.nn as nn

# Define network
class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, 10)
        self.dropout = nn.Dropout(0.2)
    
    def forward(self, x):
        x = x.view(-1, 784)
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

model = Net()
print(model)
```

### Using Sequential

```python
import torch.nn as nn

model = nn.Sequential(
    nn.Linear(784, 256),
    nn.ReLU(),
    nn.Dropout(0.2),
    nn.Linear(256, 128),
    nn.ReLU(),
    nn.Linear(128, 10)
)
```

### Pre-built Models

```python
import torchvision.models as models

# Load pretrained model
resnet = models.resnet18(pretrained=True)

# Load without weights
vgg16 = models.vgg16(weights=None)

# Modify final layer
resnet.fc = nn.Linear(resnet.fc.in_features, 10)
```

---

## Training

### Basic Training Loop

```python
import torch
import torch.nn as nn
import torch.optim as optim

# Prepare data
x_train, y_train = ...

# Create model
model = Net()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
for epoch in range(10):
    for i in range(len(x_train)):
        # Forward pass
        outputs = model(x_train[i])
        loss = criterion(outputs, y_train[i])
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")
```

### Training with DataLoader

```python
from torch.utils.data import DataLoader, TensorDataset

# Create dataset
train_ds = TensorDataset(x_train, y_train)
train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)

# Training loop
for epoch in range(10):
    for batch_x, batch_y in train_loader:
        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

### Save/Load Model

```python
# Save entire model
torch.save(model, 'model.pth')
model = torch.load('model.pth')

# Save state dict (recommended)
torch.save(model.state_dict(), 'model_weights.pth')
model.load_state_dict(torch.load('model_weights.pth'))

# Save checkpoint
checkpoint = {
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': loss,
}
torch.save(checkpoint, 'checkpoint.pth')
```

---

## Data Handling

### Using DataLoader

```python
from torch.utils.data import DataLoader

# From tensors
train_ds = TensorDataset(x_train, y_train)
train_loader = DataLoader(
    train_ds, 
    batch_size=32, 
    shuffle=True,
    num_workers=4,
    pin_memory=True
)

# Iterate
for batch_x, batch_y in train_loader:
    print(batch_x.shape, batch_y.shape)
```

### Custom Dataset

```python
from torch.utils.data import Dataset

class MyDataset(Dataset):
    def __init__(self, data, labels):
        self.data = data
        self.labels = labels
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]

dataset = MyDataset(x_train, y_train)
loader = DataLoader(dataset, batch_size=32)
```

### Image Data

```python
from torchvision import transforms
from torchvision.datasets import ImageFolder

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
])

train_ds = ImageFolder('train/', transform=transform)
train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
```

---

## GPU Support

### Move to GPU

```python
import torch

# Check GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Move model to GPU
model = Net().to(device)

# Move data to GPU
x = x.to(device)
y = y.to(device)

# Or use pin_memory for faster data transfer
train_loader = DataLoader(
    dataset, 
    batch_size=32, 
    pin_memory=True
)
```

### Multiple GPUs

```python
# Wrap model with DataParallel
model = nn.DataParallel(model)

# Or use DistributedDataParallel
```

---

## Model Saving/Loading

### Save Model

```python
# Save entire model
torch.save(model, 'model.pth')

# Save state dict (recommended)
torch.save(model.state_dict(), 'model.pth')

# Save checkpoint
torch.save({
    'epoch': 10,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': 0.5,
}, 'checkpoint.pth')
```

### Load Model

```python
# Load entire model
model = torch.load('model.pth')

# Load state dict
model = Net()
model.load_state_dict(torch.load('model.pth'))

# Load checkpoint
checkpoint = torch.load('checkpoint.pth')
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
```

---

## Transfer Learning

### Using Pretrained Models

```python
import torchvision.models as models

# Load pretrained ResNet
model = models.resnet18(pretrained=True)

# Freeze parameters
for param in model.parameters():
    param.requires_grad = False

# Replace final layer
model.fc = nn.Linear(model.fc.in_features, 10)

# Train only new layer
optimizer = optim.Adam(model.fc.parameters(), lr=0.001)

# Fine-tune
for param in model.layer4.parameters():
    param.requires_grad = True
optimizer = optim.Adam(model.parameters(), lr=0.0001)
```

### Feature Extraction

```python
# Extract features
model = models.resnet18(pretrained=True)
model = nn.Sequential(*list(model.children())[:-1])

features = model(image)
features = features.squeeze()
```

---

## Best Practices

### Performance Tips

1. **Use DataLoader with multiple workers**
   ```python
   DataLoader(dataset, num_workers=4, pin_memory=True)
   ```

2. **Use .to(device) instead of .cuda()**
   ```python
   tensor = tensor.to(device)
   ```

3. **Use no_grad() for inference**
   ```python
   with torch.no_grad():
       outputs = model(x)
   ```

4. **Set to train/eval mode**
   ```python
   model.train()
   model.eval()
   ```

### Code Structure

```python
# 1. Data loading
# 2. Model definition
# 3. Loss and optimizer
# 4. Training loop
# 5. Evaluation
# 6. Save/load
```

### Common Issues

- **Out of memory**: Reduce batch size
- **Slow training**: Use GPU, increase num_workers
- **Model not training**: Check requires_grad
- **NaN loss**: Check learning rate, data normalization

---

## Quick Reference

### Imports

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import torchvision.models as models
```

### Common Layers

```python
nn.Linear(in_features, out_features)
nn.Conv2d(in_channels, out_channels, kernel_size)
nn.MaxPool2d(kernel_size)
nn.ReLU()
nn.Softmax(dim)
nn.Dropout(p)
nn.BatchNorm2d(num_features)
nn.LSTM(input_size, hidden_size, num_layers)
```

### Training

```python
model.train()
optimizer.zero_grad()
loss.backward()
optimizer.step()

model.eval()
with torch.no_grad():
    ...
```

---

## Next Steps

- Explore [PyTorch Hub](https://pytorch.org/hub/) for models
- Learn about [PyTorch Lightning](https://www.pytorchlightning.ai/) for simplified training
- Read about [TorchScript](https://pytorch.org/docs/stable/jit.html)
