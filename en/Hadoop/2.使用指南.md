# Hadoop Usage Guide

Comprehensive guide for using Hadoop effectively.

---

## Table of Contents

- [Basic Concepts](#basic-concepts)
- [HDFS Operations](#hdfs-operations)
- [YARN Operations](#yarn-operations)
- [MapReduce Jobs](#mapreduce-jobs)
- [Managing Hadoop](#managing-hadoop)
- [Data Processing](#data-processing)
- [Best Practices](#best-practices)
- [Troubleshooting](#troubleshooting)

---

## Basic Concepts

### What is Hadoop?

Hadoop is a distributed computing platform that consists of:

| Component | Description |
|-----------|-------------|
| **HDFS** | Hadoop Distributed File System |
| **YARN** | Yet Another Resource Negotiator |
| **MapReduce** | Data processing framework |
| **Common** | Utilities and libraries |

### Key Terms

| Term | Description |
|------|-------------|
| **NameNode** | Master node managing HDFS metadata |
| **DataNode** | Worker node storing actual data |
| **ResourceManager** | Master node managing YARN resources |
| **NodeManager** | Worker node running containers |
| **Block** | Default 128MB data chunk in HDFS |

---

## HDFS Operations

### File Operations

```bash
# List files
hdfs dfs -ls /

# Create directory
hdfs dfs -mkdir -p /user/hadoop/data

# Upload file
hdfs dfs -put localfile.txt /user/hadoop/data/

# Download file
hdfs dfs -get /user/hadoop/data/file.txt ./localfile.txt

# View file content
hdfs dfs -cat /user/hadoop/data/file.txt

# View first lines
hdfs dfs -head /user/hadoop/data/file.txt

# Copy file
hdfs dfs -cp /source /destination

# Move file
hdfs dfs -mv /source /destination

# Delete file
hdfs dfs -rm /user/hadoop/data/file.txt

# Delete directory (recursive)
hdfs dfs -rm -r /user/hadoop/data
```

### Permissions

```bash
# Change permissions
hdfs dfs -chmod 755 /user/hadoop/data

# Change owner
hdfs dfs -chown hadoop:hadoop /user/hadoop/data

# Change group
hdfs dfs -chgrp groupname /user/hadoop/data
```

### File Information

```bash
# File size
hdfs dfs -du -h /user/hadoop/data

# Disk usage
hdfs dfs -df -h

# File count
hdfs dfs -count /user/hadoop

# File checksum
hdfs dfs -checksum /user/hadoop/data/file.txt
```

### Replication

```bash
# Set replication factor
hdfs dfs -setrep -w 3 /user/hadoop/data

# View replication status
hdfs fsck /user/hadoop/data -files -blocks -locations
```

---

## YARN Operations

### Application Management

```bash
# List all applications
yarn application -list

# List applications by state
yarn application -list -appStates RUNNING
yarn application -list -appStates FINISHED

# Kill application
yarn application -kill application_1234567890000_0001

# View application status
yarn application -status application_1234567890000_0001
```

### Application Logs

```bash
# View logs
yarn logs -applicationId application_1234567890000_0001

# View container logs
yarn logs -applicationId application_1234567890000_0001 -containerId container_1234567890000_0001_01_000001
```

### Node Management

```bash
# List nodes
yarn node -list

# Node details
yarn node -status node-hostname:8042

# Decommission node
# Edit $HADOOP_HOME/etc/hadoop/workers and $HADOOP_HOME/etc/hadoop/exclude
yarn rmadmin -refreshNodes
```

---

## MapReduce Jobs

### Running MapReduce Jobs

```bash
# Run example job (word count)
hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.3.jar wordcount /input /output

# Run Pi estimation
hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.3.jar pi 10 100

# Run TeraSort
hadoop jar hadoop-mapreduce-examples.jar terasort /input /output
```

### Custom MapReduce Job

```bash
# Compile and package
mvn clean package

# Run custom job
hadoop jar my-app.jar com.myapp.MyJob /input /output
```

### Job Monitoring

```bash
# List jobs
mapred job -list

# Job details
mapred job -status job_1234567890000_0001

# Job history
mapred job -history job_1234567890000_0001

# Kill job
mapred job -kill job_1234567890000_0001
```

### Job Configuration

```bash
# Set number of reducers
-D mapreduce.job.reduces=10

# Set mapper memory
-D mapreduce.map.memory.mb=2048

# Set reducer memory
-D mapreduce.reduce.memory.mb=2048

# Set queue
-D mapreduce.job.queuename=default
```

---

## Managing Hadoop

### Service Management

```bash
# Start all services
start-all.sh

# Stop all services
stop-all.sh

# Start HDFS only
start-dfs.sh

# Stop HDFS only
stop-dfs.sh

# Start YARN only
start-yarn.sh

# Stop YARN only
stop-yarn.sh
```

### Cluster Balancing

```bash
# Rebalance cluster
hdfs balancer -threshold 10
```

### Safe Mode

```bash
# Enter safe mode
hdfs dfsadmin -safemode enter

# Leave safe mode
hdfs dfsadmin -safemode leave

# Check safe mode
hdfs dfsadmin -safemode get
```

### High Availability

```bash
# Trigger failover
hdfs haadmin -transitionToActive nn1
hdfs haadmin -transitionToStandby nn2

# Check status
hdfs haadmin -getServiceState nn1
hdfs haadmin -getServiceState nn2
```

---

## Data Processing

### Spark with Hadoop

```bash
# Submit Spark job
spark-submit --master yarn --class org.apache.spark.examples.SparkPi /path/to/spark-examples.jar 100
```

### Hive with Hadoop

```bash
# Start Hive
hive

# Create table
CREATE TABLE sales (id INT, amount DOUBLE) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION '/user/hive/warehouse/sales';
```

### Pig with Hadoop

```bash
# Run Pig
pig

# Run script
pig -x mapreduce myscript.pig
```

---

## Best Practices

### HDFS Best Practices

1. **Block Size**: Use 128MB or 256MB for large files
2. **Replication**: Use 3 for production, 1 for testing
3. **Compression**: Enable compression for storage efficiency
4. **Small Files**: Avoid many small files (creates overhead)

### YARN Best Practices

1. **Resource Allocation**: Configure memory and CPU per container
2. **Queue Management**: Use multiple queues for different jobs
3. **Fair Scheduler**: Enable fair scheduling for multi-user
4. **Logging**: Configure proper log aggregation

### MapReduce Best Practices

1. **Combiners**: Use combiners to reduce data transfer
2. **Partitioners**: Use custom partitioners for skewed data
3. **Compression**: Use SequenceFile or compressed data
4. **Serialization**: Use Writable or Avro for data types

---

## Troubleshooting

### Common Issues

#### NameNode Not Starting

```bash
# Check logs
tail -f $HADOOP_HOME/logs/hadoop-hadoop-namenode-*.log

# Format NameNode (last resort)
hdfs namenode -format
```

#### DataNode Not Starting

```bash
# Check if DataNode is running
jps | grep DataNode

# Check logs
tail -f $HADOOP_HOME/logs/hadoop-hadoop-datanode-*.log

# Check disk space
hdfs dfsadmin -report
```

#### YARN ResourceManager Issues

```bash
# Check logs
tail -f $HADOOP_HOME/logs/yarn-*-resourcemanager-*.log

# Refresh queue configuration
yarn rmadmin -refreshQueues
```

### Debugging Commands

```bash
# Check cluster health
hdfs fsck /

# Check block information
hdfs fsck / -files -blocks -locations

# View NameNode UI
# http://localhost:9870

# View ResourceManager UI
# http://localhost:8088

# Check JVM settings
hdfs dfsadmin -report
```

---

## Quick Reference

### HDFS Commands

```bash
hdfs dfs -ls /           # List
hdfs dfs -mkdir /dir     # Create dir
hdfs dfs -put file /dir  # Upload
hdfs dfs -get /dir file  # Download
hdfs dfs -rm /dir        # Delete
hdfs dfs -cat /file      # View
```

### YARN Commands

```bash
yarn app -list           # List apps
yarn app -kill id        # Kill app
yarn logs -id app_id     # View logs
yarn node -list          # List nodes
```

### MapReduce Commands

```bash
mapred job -list         # List jobs
mapred job -status id    # Job status
mapred job -kill id     # Kill job
```

---

## Next Steps

- Explore [Apache Spark](https://spark.apache.org/)
- Learn [Apache Hive](https://hive.apache.org/)
- Read [Hadoop Security](https://hadoop.apache.org/docs/current/hadoop-auth/
