# PyTorch User Manual

## Basic Concepts

### Tensors

Tensors are the fundamental data structure in PyTorch, similar to NumPy arrays but with GPU support.

```python
import torch

# Create tensor from list
x = torch.tensor([1, 2, 3])

# Create random tensor
x = torch.randn(3, 3)  # 3x3 matrix with normal distribution
x = torch.zeros(3, 3)  # 3x3 matrix of zeros

# Tensor operations
y = x + 2  # Addition
z = torch.matmul(x, y)  # Matrix multiplication

# Move to GPU
if torch.cuda.is_available():
    x = x.cuda()
```

### Autograd

Automatic differentiation for computing gradients.

```python
# Create tensor with gradient tracking
x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)

# Perform operations
y = x * 2
z = y.mean()

# Compute gradients
z.backward()

# Access gradients
print(x.grad)  # tensor([0.6667, 1.3333, 2.0000])
```

---

## Building Neural Networks

### Using torch.nn

```python
import torch.nn as nn

class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(784, 256)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(256, 10)
        
    def forward(self, x):
        x = x.view(-1, 784)  # Flatten
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# Create model
model = SimpleNet()
print(model)
```

### Using torchvision Models

```python
import torchvision.models as models

# Load pretrained ResNet
resnet = models.resnet18(pretrained=True)

# Modify for different number of classes
resnet.fc = nn.Linear(resnet.fc.in_features, 10)

# Set to evaluation mode
resnet.eval()
```

---

## Training a Model

```python
import torch.optim as optim

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
for epoch in range(num_epochs):
    for batch_idx, (data, target) in enumerate(train_loader):
        # Forward pass
        output = model(data)
        loss = criterion(output, target)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
    print(f"Epoch {epoch+1}, Loss: {loss.item()}")
```

---

## Model Saving and Loading

```python
# Save model
torch.save(model.state_dict(), 'model.pth')

# Load model
model = SimpleNet()
model.load_state_dict(torch.load('model.pth'))
model.eval()

# Save entire model
torch.save(model, 'full_model.pth')

# Load entire model
model = torch.load('full_model.pth')
```

---

## GPU Training

```python
# Check if GPU is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Move model and data to GPU
model = model.to(device)
data = data.to(device)

# Use DataParallel for multiple GPUs
if torch.cuda.device_count() > 1:
    model = nn.DataParallel(model)
```

---

## Data Loading

```python
from torch.utils.data import DataLoader, Dataset

class CustomDataset(Dataset):
    def __init__(self, data, labels):
        self.data = data
        self.labels = labels
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]

# Create dataloader
train_loader = DataLoader(
    dataset=train_dataset,
    batch_size=32,
    shuffle=True,
    num_workers=4
)
```

---

## Inference

```python
# Set model to evaluation mode
model.eval()

# Disable gradient computation
with torch.no_grad():
    for data in test_loader:
        output = model(data)
        predictions = output.argmax(dim=1)
```

---

## Model Export

```python
# Export to ONNX
dummy_input = torch.randn(1, 3, 224, 224)
torch.onnx.export(model, dummy_input, 'model.onnx')
```
