# LivePortrait 使用ガイド

LivePortraitは、駆動ビデオのモーションを使用して静的なポートレートをアニメーション化する効率的なポートレートアニメーションフレームワークです。スティッチングとリターゲティング制御をサポートします。

## クイックスタート

### 基本コマンド

```bash
# ビデオ駆動アニメーション
python inference.py \
  --source_image examples/source_image.jpg \
  --driving_video examples/driving_video.mp4 \
  --result_dir ./results
```

### スティッチング付き

```bash
python inference.py \
  --source_image examples/source_image.jpg \
  --driving_video examples/driving_video.mp4 \
  --result_dir ./results \
  --stitching
```

### 目/唇リターゲティング付き

```bash
python inference.py \
  --source_image examples/source_image.jpg \
  --driven_audio examples/driven_audio.wav \
  --result_dir ./results \
  --eye_retargeting 0.5 \
  --lip_retargeting 0.5
```

## コマンドパラメータ

### 入力パラメータ

| パラメータ | 説明 | デフォルト |
|------------|------|------------|
| `--source_image` | 入力ポートレート画像パス | 必須 |
| `--driving_video` | 駆動ビデオパス | オプション |
| `--driven_audio` | 駆動オーディオパス | オプション |
| `--result_dir` | 出力ディレクトリ | ./results |

### 処理パラメータ

| パラメータ | 説明 | デフォルト |
|------------|------|------------|
| `--stitching` | スティッチングを有効化 | False |
| `--eye_retargeting` | 目リターゲティング (0-1) | 0 |
| `--lip_retargeting` | 唇リターゲティング (0-1) | 0 |
| `--face_parser` | 顔解析を有効化 | True |

### パフォーマンスパラメータ

| パラメータ | 説明 | デフォルト |
|------------|------|------------|
| `--batch_size` | バッチサイズ | 1 |
| `--fps` | 出力ビデオフレームレート | 30 |

## 使用例

### 例1：基本的なビデオ駆動アニメーション

```bash
python inference.py \
  --source_image examples/source_image.jpg \
  --driving_video examples/driving_video.mp4 \
  --result_dir results/basic
```

### 例2：スティッチング付き

```bash
python inference.py \
  --source_image examples/source_image.jpg \
  --driving_video examples/driving_video.mp4 \
  --result_dir results/stitching \
  --stitching
```

### 例3：目制御

```bash
python inference.py \
  --source_image examples/source_image.jpg \
  --driving_video examples/driving_video.mp4 \
  --result_dir results/eyes \
  --eye_retargeting 0.8
```

### 例4：唇同期制御

```bash
python inference.py \
  --source_image examples/source_image.jpg \
  --driving_video examples/driving_video.mp4 \
  --result_dir results/lips \
  --lip_retargeting 0.7
```

### 例5：フル制御

```bash
python inference.py \
  --source_image examples/source_image.jpg \
  --driving_video examples/driving_video.mp4 \
  --result_dir results/full \
  --stitching \
  --eye_retargeting 0.5 \
  --lip_retargeting 0.5
```

## Python API

### 基本的な使用法

```python
from liveportrait import LivePortrait

# 初期化
lp = LivePortrait()

# ポートレートをアニメーション化
result = lp.animate(
    source_image="image.jpg",
    driving_video="video.mp4",
    stitching=True,
    eye_retargeting=0.5,
    lip_retargeting=0.5
)

print(f"ビデオが保存されました: {result}")
```

### 上級使用法

```python
from liveportrait import LivePortrait
import torch

# カスタム設定で初期化
lp = LivePortrait(
    checkpoint_path="pretrained_models/liveportrait.pth",
    device="cuda" if torch.cuda.is_available() else "cpu"
)

# フル制御で生成
result = lp.animate(
    source_image="image.jpg",
    driving_video="video.mp4",
    stitching=True,
    eye_retargeting=0.8,
    lip_retargeting=0.8,
    output_fps=30,
    output_resolution=(512, 512)
)
```

### バッチ処理

```python
from liveportrait import LivePortrait
import os

lp = LivePortrait()

# 複数画像を処理
source_dir = "source_images/"
driving_video = "driving.mp4"

for image_file in os.listdir(source_dir):
    if image_file.endswith(('.jpg', '.png')):
        result = lp.animate(
            source_image=os.path.join(source_dir, image_file),
            driving_video=driving_video,
            stitching=True
        )
```

## Webインターフェース

### Webデモの実行

```bash
python app.py
```

ブラウザでhttp://localhost:7860を開く

### Webインターフェース機能

1. **ソース画像アップロード** - ポートレート画像をアップロード
2. **駆動ビデオアップロード** - 駆動ビデオを選択
3. **スティッチング切替** - スティッチングの有効/無効
4. **目制御** - 目の動きの強度を調整
5. **唇制御** - 唇同期の強度を調整
6. **生成** - アニメーションを作成
7. **ダウンロード** - 結果を保存

## 機能

### 1. スティッチング

スティッチングモジュールは、生成された頭部と体をシームレスに接続します：

```bash
--stitching
```

- 頭部と体の間の滑らかな遷移
- 様々な画像スタイルに対応（リアル、油絵、彫刻、3D）

### 2. 目リターゲティング

スカラー値で目の開き度を制御 (0-1)：

```bash
--eye_retargeting 0.5
```

- 0: 目を閉じる
- 1: 目を完全に開く
- 0.5: 自然

### 3. 唇リターゲティング

唇の動きの強度を制御：

```bash
--lip_retargeting 0.5
```

- 0: 最小限の動き
- 1: 最大の動き
- 0.5: 自然

## 入力要件

### ソース画像

- 形式：JPG、PNG
- 解像度：512x512以上を推奨
- 顔：正面向き、明確
- 背景：任意

### 駆動ビデオ

- 形式：MP4、AVI
- 長さ：1〜60秒
- 解像度：任意
- 顔：明確な表情

## 出力

### ビデオ形式

- 形式：MP4
- コーデック：H.264
- 解像度：512x512
- フレームレート：30

### 出力ディレクトリ

```
results/
├── output.mp4          # 生成されたビデオ
└── (一時ファイル)
```

## トラブルシューティング

### アニメーション品質が悪い

**解決策：**
- より高品質のソース画像を使用
- 駆動ビデオに明確な顔があることを確認
- リターゲティングパラメータを調整

### 顔が検出されない

**解決策：**
- より明確なポートレート画像を使用
- 顔が見えていることを確認
- 画像形式を確認

### 処理が遅い

**解決策：**
- 出力解像度を下げる
- 必要なければスティッチングを無効化
- GPU高速化を使用

### GPUメモリ不足

**解決策：**
- バッチサイズを1に削減
- より小さい画像サイズを使用
- 他のGPUアプリケーションを閉じる

## パフォーマンスのヒント

### より高速な処理

1. 必要なければスティッチングを無効化
2. より低い出力解像度を使用
3. fpsを下げる

### より良い品質

1. スティッチングを有効化
2. 高品質ソース画像を使用
3. リターゲティングパラメータを調整

## 関連リンク

- [GitHub](https://github.com/KwaiVGI/LivePortrait)
- [論文](https://arxiv.org/abs/2407.03168)
- [デモ](https://liveportrait.github.io)
