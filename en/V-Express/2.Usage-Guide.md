# V-Express Usage Guide

V-Express generates talking head videos under the control of a reference image, audio, and V-Kps sequences.

## Quick Start

### Basic Command

```bash
python inference.py \
  --reference_image_path "./test_samples/short_case/AOC/ref.jpg" \
  --audio_path "./test_samples/short_case/AOC/aud.mp3" \
  --kps_path "./test_samples/short_case/AOC/kps.pth" \
  --output_path "./output/result.mp4" \
  --retarget_strategy "no_retarget" \
  --num_inference_steps 25
```

## Command Parameters

### Input Parameters

| Parameter | Description | Required |
|-----------|------------|----------|
| `--reference_image_path` | Reference portrait image path | Yes |
| `--audio_path` | Input audio file path (MP3/WAV) | Yes |
| `--kps_path` | V-Kps sequence file path | No |
| `--output_path` | Output video path | Yes |

### Retargeting Strategies

| Strategy | Description |
|----------|-------------|
| `no_retarget` | Same person's picture and video (best results) |
| `fix_face` | Picture and any audio (lip-sync only) |
| `offset_retarget` | Different person's picture with slight facial motion |
| `naive_retarget` | Different person's picture with full retargeting |

### Processing Parameters

| Parameter | Description | Default |
|-----------|------------|---------|
| `--retarget_strategy` | Face retargeting strategy | no_retarget |
| `--num_inference_steps` | Number of inference steps | 25 |
| `--reference_attention_weight` | Reference image weight (0.9-1.0) | 1.0 |
| `--audio_attention_weight` | Audio weight (1.0-3.0) | 1.0 |
| `--save_gpu_memory` | Enable memory saving mode | False |

### Recommended Parameter Ranges

- `reference_attention_weight`: 0.9-1.0
- `audio_attention_weight`: 1.0-3.0

## Usage Examples

### Scenario 1: Same Person (Best Quality)

When you have a picture of person A and a talking video of person A:

```bash
python inference.py \
  --reference_image_path "./test_samples/short_case/AOC/ref.jpg" \
  --audio_path "./test_samples/short_case/AOC/aud.mp3" \
  --kps_path "./test_samples/short_case/AOC/kps.pth" \
  --output_path "./output/talk_AOC_no_retarget.mp4" \
  --retarget_strategy "no_retarget" \
  --num_inference_steps 25
```

### Scenario 2: Any Audio (Lip Sync Only)

When you only have a picture and any talking audio:

```bash
python inference.py \
  --reference_image_path "./test_samples/short_case/tys/ref.jpg" \
  --audio_path "./test_samples/short_case/tys/aud.mp3" \
  --output_path "./output/talk_tys_fix_face.mp4" \
  --retarget_strategy "fix_face" \
  --num_inference_steps 25
```

### Scenario 3: Different Person with Facial Motion

When you have a picture of person A and talking video of person B:

```bash
python inference.py \
  --reference_image_path "./test_samples/short_case/tys/ref.jpg" \
  --audio_path "./test_samples/short_case/tys/aud.mp3" \
  --kps_path "./test_samples/short_case/tys/kps.pth" \
  --output_path "./output/talk_tys_offset_retarget.mp4" \
  --retarget_strategy "offset_retarget" \
  --num_inference_steps 25
```

### Scenario 4: Custom Attention Weights

Adjust weights for different effects:

```bash
python inference.py \
  --reference_image_path "./test_samples/short_case/10/ref.jpg" \
  --audio_path "./test_samples/short_case/10/aud.mp3" \
  --output_path "./output/talk_10_weighted.mp4" \
  --retarget_strategy "fix_face" \
  --reference_attention_weight 0.95 \
  --audio_attention_weight 3.0
```

### Scenario 5: Long Audio (Memory Optimized)

For longer audio files (30+ seconds):

```bash
python inference.py \
  --reference_image_path "./test_samples/short_case/AOC/ref.jpg" \
  --audio_path "./test_samples/short_case/AOC/long_audio.mp3" \
  --kps_path "./test_samples/short_case/AOC/AOC_raw_kps.pth" \
  --output_path "./output/long_video.mp4" \
  --retarget_strategy "no_retarget" \
  --num_inference_steps 25 \
  --reference_attention_weight 1.0 \
  --audio_attention_weight 1.0 \
  --save_gpu_memory
```

## Extract V-Kps Sequence

If you have a target video, extract V-Kps sequence:

```bash
python scripts/extract_kps_sequence_and_audio.py \
  --video_path "./test_samples/short_case/AOC/gt.mp4" \
  --kps_sequence_save_path "./test_samples/short_case/AOC/kps.pth" \
  --audio_save_path "./test_samples/short_case/AOC/aud.mp3"
```

## Input Requirements

### Reference Image

- Format: JPG, PNG
- Resolution: 512x512 or higher (square recommended)
- Face: Front-facing, clear, no occlusion
- Background: Simple preferred

### Audio

- Format: MP3, WAV
- Duration: 1-60 seconds (longer with --save_gpu_memory)
- Sample rate: 16000-48000 Hz
- Speech: Clear, minimal noise

### V-Kps Sequence

- Format: PyTorch (.pth)
- Generated from target video using provided script

## Output

### Video Format

- Format: MP4
- Codec: H.264
- Resolution: Same as input or default 512x512
- Frame rate: 24-30

## Troubleshooting

### Poor Video Quality

**Solutions:**
- Use higher quality reference image (512x512+)
- Increase inference steps (25-30)
- Adjust attention weights

### Lip Sync Issues

**Solutions:**
- Increase `audio_attention_weight` (1.5-3.0)
- Use higher quality audio
- Ensure audio has clear speech

### Face Deformation

**Solutions:**
- Use similar pose in target video
- Try different retarget strategy
- Use no_retarget for same person

### GPU Out of Memory

**Solutions:**
- Enable `--save_gpu_memory`
- Reduce inference steps
- Use shorter audio

### Processing Too Slow

**Solutions:**
- Reduce inference steps (15-20)
- Use smaller model
- Disable unused features

## Best Practices

1. **Same Person Videos**: Use `no_retarget` strategy for best results
2. **Different Person**: Choose target video with similar pose
3. **Image Quality**: Use clear 512x512 square face images
4. **Audio Quality**: Use clear speech audio
5. **Weight Tuning**: Adjust attention weights for different effects

## Related Links

- [GitHub](https://github.com/tencent-ailab/V-Express)
- [HuggingFace](https://huggingface.co/tk93/V-Express)
- [Paper](https://arxiv.org/abs/2406.02511)
- [Project Page](https://tenvence.github.io/p/v-express/)
