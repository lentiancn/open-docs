# V-Express ユーザーガイド

V-Expressは、参照画像、音声、V-Kps配列制御下でアバター動画を生成します。

## クイックスタート

### 基本的なコマンド

```bash
python inference.py \
  --reference_image_path "./test_samples/short_case/AOC/ref.jpg" \
  --audio_path "./test_samples/short_case/AOC/aud.mp3" \
  --kps_path "./test_samples/short_case/AOC/kps.pth" \
  --output_path "./output/result.mp4" \
  --retarget_strategy "no_retarget" \
  --num_inference_steps 25
```

## コマンドパラメータ

### 入力パラメータ

| パラメータ | 説明 | 必須 |
|-----------|------|------|
| `--reference_image_path` | 参照ポートレート画像パス | ○ |
| `--audio_path` | 入力音声ファイルパス (MP3/WAV) | ○ |
| `--kps_path` | V-Kps配列ファイルパス | - |
| `--output_path` | 出力動画パス | ○ |

### リターゲット戦略

| 戦略 | 説明 |
|------|------|
| `no_retarget` | 同一人物の画像と動画（最高品質） |
| `fix_face` | 任意の画像と音声（唇同期のみ） |
| `offset_retarget` | 異なる人物の画像、わずかな顔の動き付き |
| `naive_retarget` | 異なる人物の画像、完全リターゲット |

### 処理パラメータ

| パラメータ | 説明 | デフォルト |
|-----------|------|-----------|
| `--retarget_strategy` | 顔リターゲット戦略 | no_retarget |
| `--num_inference_steps` | 推論ステップ数 | 25 |
| `--reference_attention_weight` | 参照画像重み (0.9-1.0) | 1.0 |
| `--audio_attention_weight` | 音声重み (1.0-3.0) | 1.0 |
| `--save_gpu_memory` | メモリ節約モードを有効化 | False |

### 推奨パラメータ範囲

- `reference_attention_weight`：0.9-1.0
- `audio_attention_weight`：1.0-3.0

## 使用例

### シナリオ1：同一人物（最高品質）

人物Aの画像と人物Aの会話動画がある場合：

```bash
python inference.py \
  --reference_image_path "./test_samples/short_case/AOC/ref.jpg" \
  --audio_path "./test_samples/short_case/AOC/aud.mp3" \
  --kps_path "./test_samples/short_case/AOC/kps.pth" \
  --output_path "./output/talk_AOC_no_retarget.mp4" \
  --retarget_strategy "no_retarget" \
  --num_inference_steps 25
```

### シナリオ2：任意の音声（唇同期のみ）

画像と任意の会話音声しかない場合：

```bash
python inference.py \
  --reference_image_path "./test_samples/short_case/tys/ref.jpg" \
  --audio_path "./test_samples/short_case/tys/aud.mp3" \
  --output_path "./output/talk_tys_fix_face.mp4" \
  --retarget_strategy "fix_face" \
  --num_inference_steps 25
```

### シナリオ3：異なる人物で顔の動き付き

人物Aの画像と人物Bの会話動画がある場合：

```bash
python inference.py \
  --reference_image_path "./test_samples/short_case/tys/ref.jpg" \
  --audio_path "./test_samples/short_case/tys/aud.mp3" \
  --kps_path "./test_samples/short_case/tys/kps.pth" \
  --output_path "./output/talk_tys_offset_retarget.mp4" \
  --retarget_strategy "offset_retarget" \
  --num_inference_steps 25
```

### シナリオ4：カスタムアテンション重み

異なる効果を得る：

```bash
ために重みを調整python inference.py \
  --reference_image_path "./test_samples/short_case/10/ref.jpg" \
  --audio_path "./test_samples/short_case/10/aud.mp3" \
  --output_path "./output/talk_10_weighted.mp4" \
  --retarget_strategy "fix_face" \
  --reference_attention_weight 0.95 \
  --audio_attention_weight 3.0
```

### シナリオ5：長い音声（メモリ最適化）

より長い音声ファイル（30秒以上）の場合：

```bash
python inference.py \
  --reference_image_path "./test_samples/short_case/AOC/ref.jpg" \
  --audio_path "./test_samples/short_case/AOC/long_audio.mp3" \
  --kps_path "./test_samples/short_case/AOC/AOC_raw_kps.pth" \
  --output_path "./output/long_video.mp4" \
  --retarget_strategy "no_retarget" \
  --num_inference_steps 25 \
  --reference_attention_weight 1.0 \
  --audio_attention_weight 1.0 \
  --save_gpu_memory
```

## V-Kps配列の抽出

ターゲット動画がある場合は、V-Kps配列を抽出：

```bash
python scripts/extract_kps_sequence_and_audio.py \
  --video_path "./test_samples/short_case/AOC/gt.mp4" \
  --kps_sequence_save_path "./test_samples/short_case/AOC/kps.pth" \
  --audio_save_path "./test_samples/short_case/AOC/aud.mp3"
```

## 入力要件

### 参照画像

- 形式：JPG、PNG
- 解像度：512x512以上（正方形推奨）
- 顔：正面を向いている、清晰、遮蔽物なし
- 背景：シンプルが望ましい

### 音声

- 形式：MP3、WAV
- 長さ：1-60秒（--save_gpu_memory使用時は更长）
- サンプルレート：16000-48000 Hz
-  Speech：清晰、噪声最小

### V-Kps配列

- 形式：PyTorch (.pth)
- 提供されたスクリプトを使用してターゲット動画から生成

## 出力

### 動画形式

- 形式：MP4
- コーデック：H.264
- 解像度：入力と同じまたはデフォルト512x512
- フレームレート：24-30

## トラブルシューティング

### 動画品質が悪い

**解決策：**
- より高品質な参照画像を使用（512x512+）
- 推論ステップ数を増やす（25-30）
- アテンション重みを調整

### 唇同期の問題

**解決策：**
- `audio_attention_weight`を増やす（1.5-3.0）
- より高品質な音声を使用
- 音声が明確なSpeechを含むことを確認

### 顔の変形

**解決策：**
- ターゲット動画で類似ポーズを使用
- 異なるリターゲット戦略を試す
- 同一人物の場合はno_retargetを使用

### GPUメモリ不足

**解決策：**
- `--save_gpu_memory`を有効化
- 推論ステップ数を減らす
- より短い音声を使用

### 処理が遅すぎる

**解決策：**
- 推論ステップ数を減らす（15-20）
- より小さいモデルを使用
- 未使用の機能を無効化

## ベストプラクティス

1. **同一人物の動画**：`no_retarget`戦略を使用して最高品質の結果を得る
2. **異なる人物**：参照顔のポーズに類似したターゲット動画を選択
3. **画像品質**：清晰な512x512正方形顔画像を使用
4. **音声品質**：明確なSpeech音声を使用
5. **重み調整**：異なる効果のためにアテンション重みを調整

## 関連リンク

- [GitHub](https://github.com/tencent-ailab/V-Express)
- [HuggingFace](https://huggingface.co/tk93/V-Express)
- [論文](https://arxiv.org/abs/2406.02511)
- [プロジェクトページ](https://tenvence.github.io/p/v-express/)
