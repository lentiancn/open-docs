# TensorFlow Usage Guide

Comprehensive guide for using TensorFlow effectively.

---

## Table of Contents

- [Basic Concepts](#basic-concepts)
- [Tensor Operations](#tensor-operations)
- [Neural Networks](#neural-networks)
- [Data Processing](#data-processing)
- [Model Training](#model-training)
- [Model Evaluation](#model-evaluation)
- [Transfer Learning](#transfer-learning)
- [Saving and Loading Models](#saving-and-loading-models)
- [GPU Usage](#gpu-usage)
- [Best Practices](#best-practices)

---

## Basic Concepts

### What is TensorFlow?

TensorFlow is an open-source machine learning framework developed by Google. It provides a comprehensive ecosystem for building and deploying ML models.

### Key Terms

| Term | Description |
|------|-------------|
| **Tensor** | Multi-dimensional array |
| **TensorFlow** | Computational graph |
| **Eager Execution** | Immediate operation execution |
| **Keras** | High-level API (included in TF 2.x) |
| **TFLite** | TensorFlow Lite for mobile |
| **TF.js** | TensorFlow for JavaScript |

---

## Tensor Operations

### Creating Tensors

```python
import tensorflow as tf

# Constant tensor
scalar = tf.constant(5)           # 0D tensor
vector = tf.constant([1, 2, 3])   # 1D tensor
matrix = tf.constant([[1, 2], [3, 4]])  # 2D tensor

# Tensor from NumPy
import numpy as np
arr = np.array([1, 2, 3])
tensor = tf.constant(arr)

# Ones and zeros
ones = tf.ones((3, 3))
zeros = tf.zeros((2, 4))

# Random tensors
random = tf.random.normal((3, 3))
```

### Tensor Operations

```python
import tensorflow as tf

a = tf.constant([[1, 2], [3, 4]])
b = tf.constant([[5, 6], [7, 8]])

# Addition
c = tf.add(a, b)
c = a + b

# Multiplication
# Element-wise
c = tf.multiply(a, b)
c = a * b

# Matrix multiplication
c = tf.matmul(a, b)
c = a @ b

# Other operations
c = tf.reduce_sum(a)       # Sum
c = tf.reduce_mean(a)      # Mean
c = tf.reshape(a, (4, 1))   # Reshape
c = tf.transpose(a)       # Transpose
```

---

## Neural Networks

### Using Keras Sequential API

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Create model
model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(784,)),
    layers.Dense(32, activation='relu'),
    layers.Dense(10, activation='softmax')
])

# Compile model
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Model summary
model.summary()
```

### Using Keras Functional API

```python
import tensorflow as tf
from tensorflow.keras import layers, Model

# Input layer
inputs = layers.Input(shape=(784,))

# Dense layers
x = layers.Dense(64, activation='relu')(inputs)
x = layers.Dense(32, activation='relu')(x)

# Output layer
outputs = layers.Dense(10, activation='softmax')(x)

# Create model
model = Model(inputs=inputs, outputs=outputs)
```

### Custom Layer

```python
import tensorflow as tf
from tensorflow.keras import layers

class CustomDense(layers.Layer):
    def __init__(self, units, **kwargs):
        super().__init__(**kwargs)
        self.units = units

    def build(self, input_shape):
        self.kernel = self.add_weight(
            shape=(input_shape[-1], self.units),
            initializer='glorot_uniform',
            trainable=True
        )

    def call(self, inputs):
        return tf.matmul(inputs, self.kernel)

    def get_config(self):
        config = super().get_config()
        config.update({'units': self.units})
        return config
```

---

## Data Processing

### Using tf.data

```python
import tensorflow as tf

# From NumPy arrays
train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))
test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))

# Batch and shuffle
train_ds = train_ds.shuffle(10000).batch(32).prefetch(tf.data.AUTOTUNE)

# From files
# For images
train_ds = tf.keras.utils.image_dataset_from_directory(
    'data/train',
    batch_size=32,
    image_size=(256, 256)
)

# For CSV
train_ds = tf.data.experimental.make_csv_dataset(
    'data.csv',
    batch_size=32,
    num_epochs=1
)
```

### Data Augmentation

```python
from tensorflow.keras import layers

data_augmentation = keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
    layers.RandomContrast(0.1),
])

# Use in model
model = keras.Sequential([
    layers.Input(shape=(256, 256, 3)),
    data_augmentation,
    layers.Dense(64),
])
```

---

## Model Training

### Basic Training

```python
import tensorflow as tf

# Prepare data
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train = x_train.reshape(-1, 784).astype('float32') / 255.0

# Create model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(512, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train
history = model.fit(
    x_train, y_train,
    epochs=10,
    batch_size=32,
    validation_split=0.1
)

# Evaluate
model.evaluate(x_test, y_test)
```

### Callbacks

```python
from tensorflow.keras import callbacks

early_stopping = callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

model_checkpoint = callbacks.ModelCheckpoint(
    'best_model.keras',
    monitor='val_accuracy',
    save_best_only=True
)

reduce_lr = callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.2,
    patience=3
)

# Use callbacks
model.fit(
    x_train, y_train,
    epochs=50,
    callbacks=[early_stopping, model_checkpoint, reduce_lr]
)
```

### Custom Training Loop

```python
import tensorflow as tf

# Prepare data
train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)

# Define model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(10, activation='softmax')
])

# Define loss and optimizer
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()
optimizer = tf.keras.optimizers.Adam()

# Training loop
@tf.function
def train_step(x, y):
    with tf.GradientTape() as tape:
        predictions = model(x, training=True)
        loss = loss_fn(y, predictions)
    
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    return loss

# Train
for epoch in range(10):
    for x, y in train_ds:
        loss = train_step(x, y)
    print(f"Epoch {epoch+1}, Loss: {loss.numpy()}")
```

---

## Model Evaluation

### Evaluate Model

```python
# Basic evaluation
loss, accuracy = model.evaluate(x_test, y_test)
print(f"Loss: {loss:.4f}")
print(f"Accuracy: {accuracy:.4f}")

# Detailed evaluation
predictions = model.predict(x_test)

# For classification
from sklearn.metrics import classification_report
import numpy as np

y_pred = np.argmax(predictions, axis=1)
print(classification_report(y_test, y_pred))

# Confusion matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)
```

### Custom Metrics

```python
class CustomMetric(tf.keras.metrics.Metric):
    def __init__(self, name='custom_metric', **kwargs):
        super().__init__(name=name, **kwargs)
        self.total = self.add_weight(name='total', initializer='zeros')
        self.count = self.add_weight(name='count', initializer='zeros')

    def update_state(self, y_true, y_pred, sample_weight=None):
        values = tf.cast(y_true == y_pred, tf.float32)
        self.total.assign_add(tf.reduce_sum(values))
        self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))

    def result(self):
        return self.total / self.count

    def reset_state(self):
        self.total.assign(0.0)
        self.count.assign(0.0)
```

---

## Transfer Learning

### Using Pre-trained Models

```python
import tensorflow as tf

# Load pre-trained model (e.g., MobileNetV2)
base_model = tf.keras.applications.MobileNetV2(
    input_shape=(224, 224, 3),
    include_top=False,
    weights='imagenet'
)

# Freeze base model
base_model.trainable = False

# Add classification head
inputs = tf.keras.Input(shape=(224, 224, 3))
x = base_model(inputs, training=False)
x = tf.keras.layers.GlobalAveragePooling2D()(x)
outputs = tf.keras.layers.Dense(10)(x)

model = tf.keras.Model(inputs, outputs)

# Fine-tune last layers
base_model.trainable = True
for layer in base_model.layers[:-20]:
    layer.trainable = False

model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss='mse')
```

### Using tf.keras.applications

```python
# Available models
# ResNet50, VGG16, VGG19, EfficientNet, etc.

from tensorflow.keras.applications import ResNet50

model = ResNet50(weights='imagenet')

# Use for feature extraction
features = model.predict(images)
```

---

## Saving and Loading Models

### Save/Load Keras Model

```python
# Save entire model
model.save('my_model.keras')
model = tf.keras.models.load_model('my_model.keras')

# Save weights only
model.save_weights('my_model_weights.weights.h5')
model.load_weights('my_model_weights.weights.h5')

# Save in SavedModel format (TensorFlow serving)
model.save('saved_model/')
model = tf.keras.models.load_model('saved_model/')
```

### Save/Load with Custom Objects

```python
# Save with custom objects
model.save('model.keras', custom_objects={'CustomDense': CustomDense})
model = tf.keras.models.load_model('model.keras', custom_objects={'CustomDense': CustomDense})
```

### Export to TFLite

```python
# Convert to TensorFlow Lite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Save
with open('model.tflite', 'wb') as f:
    f.write(tflite_model)
```

---

## GPU Usage

### Check GPU

```python
import tensorflow as tf

# List GPUs
gpus = tf.config.list_physical_devices('GPU')
print(f"GPUs available: {len(gpus)}")

for gpu in gpus:
    print(f"GPU: {gpu}")

# Check GPU memory
if gpus:
    print(tf.config.experimental.get_device_details(gpus[0]))
```

### GPU Memory Growth

```python
import tensorflow as tf

gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print("Memory growth enabled")
    except RuntimeError as e:
        print(f"Error: {e}")
```

### Mixed Precision

```python
from tensorflow.keras import mixed_precision
mixed_precision.set_global_policy('mixed_float16')

# Check policy
print(mixed_precision.global_policy())
```

---

## Best Practices

### Performance Tips

1. **Use tf.data for data pipeline**
   ```python
   dataset = dataset.prefetch(tf.data.AUTOTUNE)
   dataset = dataset.cache()
   ```

2. **Enable XLA compilation**
   ```python
   @tf.function(jit_compile=True)
   def train_step(x, y):
       ...
   ```

3. **Use mixed precision**
   ```python
   mixed_precision.set_global_policy('mixed_float16')
   ```

4. **Enable memory growth**
   ```python
   tf.config.experimental.set_memory_growth(gpu, True)
   ```

### Code Organization

```python
# Use clear structure
# 1. Data loading
# 2. Model definition
# 3. Model compilation
# 4. Training
# 5. Evaluation

# Use callbacks for common tasks
# - EarlyStopping
# - ModelCheckpoint
# - TensorBoard
# - ReduceLROnPlateau
```

---

## Quick Reference

### Common Imports

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
```

### Model Creation

```python
model = keras.Sequential([
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])
```

### Training

```python
model.fit(x_train, y_train, epochs=10, batch_size=32)
model.evaluate(x_test, y_test)
model.predict(x_new)
```

### Layers

```python
layers.Dense(64, activation='relu')
layers.Conv2D(32, (3, 3), activation='relu')
layers.MaxPooling2D((2, 2))
layers.Dropout(0.5)
layers.BatchNormalization()
layers.LSTM(64)
layers.Embedding(1000, 128)
```

---

## Next Steps

- Explore [TensorFlow Hub](https://tfhub.dev/) for pre-trained models
- Learn about [TensorFlow Extended (TFX)](https://www.tensorflow.org/tfx) for production ML
- Read about [TensorFlow Lite](https://www.tensorflow.org/lite) for mobile/edge
