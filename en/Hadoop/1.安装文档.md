# Hadoop Installation Guide

This guide provides detailed instructions for installing Hadoop on all major platforms.

---

## Table of Contents

- [Installation Methods](#installation-methods)
- [Single Node Setup (Learning)](#single-node-setup-learning)
- [Multi-Node Cluster Setup](#multi-node-cluster-setup)
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Configuration](#configuration)
- [Starting Hadoop](#starting-hadoop)
- [Verification](#verification)
- [Uninstall Hadoop](#uninstall-hadoop)

---

## Installation Methods

| Method | Description | Best For |
|--------|-------------|----------|
| **Single Node** | All services on one machine | Learning, testing |
| **Pseudo-Distributed** | Simulated cluster on one machine | Development |
| **Fully Distributed** | Multiple machines | Production |

---

## Prerequisites

### System Requirements

| Resource | Minimum | Recommended |
|----------|---------|-------------|
| CPU | 2 cores | 4+ cores |
| RAM | 4 GB | 8+ GB |
| Disk | 10 GB | 100+ GB |

### Required Software

- Java 8 or 11 (JDK 8 or 11)
- SSH
- Linux operating system (Ubuntu, CentOS, etc.)

### Install Java

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install -y openjdk-11-jdk

# Set JAVA_HOME
echo 'export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64' >> ~/.bashrc
source ~/.bashrc
```

**CentOS/RHEL:**
```bash
sudo yum install -y java-11-openjdk java-11-openjdk-devel

# Set JAVA_HOME
echo 'export JAVA_HOME=/usr/lib/jvm/java-11-openjdk' >> ~/.bashrc
source ~/.bashrc
```

**macOS:**
```bash
# Install Homebrew if not already installed
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# Install OpenJDK
brew install openjdk@11

# Set JAVA_HOME
echo 'export JAVA_HOME=/usr/local/opt/openjdk@11' >> ~/.bashrc
source ~/.bashrc
```

### Install SSH

```bash
# Ubuntu/Debian
sudo apt install -y openssh-server openssh-client

# CentOS/RHEL
sudo yum install -y openssh-server openssh-clients

# macOS (usually pre-installed)
```

---

## Single Node Setup (Learning)

### Download Hadoop

```bash
# Download latest stable version (3.4.3)
wget https://dlcdn.apache.org/hadoop/common/hadoop-3.4.3/hadoop-3.4.3.tar.gz

# Or use a mirror
curl -O https://dlcdn.apache.org/hadoop/common/hadoop-3.4.3/hadoop-3.4.3.tar.gz

# Extract
tar -xzf hadoop-3.4.3.tar.gz

# Move to /opt
sudo mv hadoop-3.4.3 /opt/hadoop

# Set permissions
sudo chown -R $USER:$USER /opt/hadoop
```

### Configure Environment Variables

```bash
# Add to ~/.bashrc or ~/.zshrc
cat >> ~/.bashrc << 'EOF'

# Hadoop Environment Variables
export HADOOP_HOME=/opt/hadoop
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export HADOOP_YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
EOF

# Apply changes
source ~/.bashrc
```

### Configure Hadoop

#### 1. core-site.xml

```bash
mkdir -p /opt/hadoop/tmp
nano $HADOOP_HOME/etc/hadoop/core-site.xml
```

```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
        <description>NameNode URI</description>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/opt/hadoop/tmp</value>
    </property>
</configuration>
```

#### 2. hdfs-site.xml

```bash
nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml
```

```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.name.dir</name>
        <value>file:///opt/hadoop/hdfs/namenode</value>
    </property>
    <property>
        <name>dfs.data.dir</name>
        <value>file:///opt/hadoop/hdfs/datanode</value>
    </property>
</configuration>
```

#### 3. mapred-site.xml

```bash
nano $HADOOP_HOME/etc/hadoop/mapred-site.xml
```

```xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.application.classpath</name>
        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
    </property>
</configuration>
```

#### 4. yarn-site.xml

```bash
nano $HADOOP_HOME/etc/hadoop/yarn-site.xml
```

```xml
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
</configuration>
```

### Configure SSH for Passwordless Login

```bash
# Generate SSH key
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa

# Add to authorized keys
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

# Set permissions
chmod 0600 ~/.ssh/authorized_keys

# Test SSH
ssh localhost
```

---

## Multi-Node Cluster Setup

### Architecture

```
Master Node (NameNode + ResourceManager)
    |
    +-- Worker Node 1 (DataNode + NodeManager)
    +-- Worker Node 2 (DataNode + NodeManager)
    +-- Worker Node 3 (DataNode + NodeManager)
```

### Setup Steps

#### 1. Configure Master Node

```bash
# On master node, create workers file
nano $HADOOP_HOME/etc/hadoop/workers
```

Add worker hostnames:
```
worker1
worker2
worker3
```

#### 2. Configure core-site.xml (All Nodes)

```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://master:9000</value>
    </property>
</configuration>
```

#### 3. Configure hdfs-site.xml (All Nodes)

```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///opt/hadoop/hdfs/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///opt/hadoop/hdfs/datanode</value>
    </property>
</configuration>
```

#### 4. Configure yarn-site.xml (All Nodes)

```xml
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>master</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
</configuration>
```

#### 5. Distribute Configuration

```bash
# Copy to all worker nodes
scp $HADOOP_HOME/etc/hadoop/* worker1:/opt/hadoop/etc/hadoop/
scp $HADOOP_HOME/etc/hadoop/* worker2:/opt/hadoop/etc/hadoop/
scp $HADOOP_HOME/etc/hadoop/* worker3:/opt/hadoop/etc/hadoop/
```

#### 6. Set Up SSH Passwordless (All Nodes)

```bash
# On master, generate and copy key to workers
ssh-copy-id worker1
ssh-copy-id worker2
ssh-copy-id worker3
```

---

## Starting Hadoop

### Format NameNode (First Time Only)

```bash
$HADOOP_HOME/bin/hdfs namenode -format
```

### Start HDFS

```bash
# Start NameNode and DataNode
$HADOOP_HOME/sbin/start-dfs.sh
```

### Start YARN

```bash
# Start ResourceManager and NodeManager
$HADOOP_HOME/sbin/start-yarn.sh
```

### Start All Services

```bash
# Start all services
$HADOOP_HOME/sbin/start-all.sh
```

---

## Verification

### Check Running Java Processes

```bash
jps
```

Expected output:
```
NameNode
DataNode
ResourceManager
NodeManager
SecondaryNameNode
```

### Check Web Interfaces

| Service | URL |
|---------|-----|
| NameNode | http://localhost:9870 |
| ResourceManager | http://localhost:8088 |

### Test Hadoop

```bash
# Create directory in HDFS
hdfs dfs -mkdir -p /user/$USER

# Copy file to HDFS
echo "Hello Hadoop" > test.txt
hdfs dfs -put test.txt /user/$USER/

# List files
hdfs dfs -ls /user/$USER/

# Read file
hdfs dfs -cat /user/$USER/test.txt

# Run example word count
hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.3.jar wordcount /user/$USER/test.txt output

# View output
hdfs dfs -cat output/part-r-00000
```

---

## Hadoop Commands

### HDFS Commands

```bash
# List files
hdfs dfs -ls /

# Create directory
hdfs dfs -mkdir -p /user/hadoop

# Upload file
hdfs dfs -put localfile /hdfs/path

# Download file
hdfs dfs -get /hdfs/path localfile

# Remove file
hdfs dfs -rm /hdfs/path

# View file content
hdfs dfs -cat /hdfs/path

# Check disk usage
hdfs dfs -du -h /
```

### YARN Commands

```bash
# List applications
yarn application -list

# Kill application
yarn application -kill application_id

# View logs
yarn logs -applicationId application_id
```

---

## Uninstall Hadoop

```bash
# Stop all services
$HADOOP_HOME/sbin/stop-all.sh

# Remove Hadoop directory
sudo rm -rf /opt/hadoop

# Remove data directory
sudo rm -rf /opt/hadoop/tmp
sudo rm -rf /opt/hadoop/hdfs

# Remove environment variables from ~/.bashrc
# Remove the Hadoop environment variables you added
```

---

## Version Information

### Current Stable Version

- **Hadoop 3.4.3** (Latest stable release)
- Requires Java 8 or 11

### Version History

| Version | Release Date | Key Features |
|---------|-------------|--------------|
| 3.4.x | 2024 | Latest stable |
| 3.3.x | 2021 | Many improvements |
| 3.2.x | 2019 | Long term support |
| 2.x | 2012 | Legacy stable |

---

## Next Steps

- [Usage Guide](./2.使用指南.md)
- [Hadoop Official Documentation](https://hadoop.apache.org/docs/)
- [Apache Hadoop Wiki](https://wiki.apache.org/hadoop/)
