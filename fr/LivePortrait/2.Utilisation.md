# Guide d'utilisation de LivePortrait

LivePortrait est un framework efficace d'animation de portraits qui anime les portraits statiques en utilisant le mouvement des vidéos sources. Il supporte le contrôle du stitching et du re-targeting.

## Démarrage rapide

### Commande de base

```bash
# Animation pilotée par vidéo
python inference.py \
  --source_image examples/source_image.jpg \
  --driving_video examples/driving_video.mp4 \
  --result_dir ./results
```

### Avec stitching

```bash
python inference.py \
  --source_image examples/source_image.jpg \
  --driving_video examples/driving_video.mp4 \
  --result_dir ./results \
  --stitching
```

### Avec re-targeting yeux/lèvres

```bash
python inference.py \
  --source_image examples/source_image.jpg \
  --driven_audio examples/driven_audio.wav \
  --result_dir ./results \
  --eye_retargeting 0.5 \
  --lip_retargeting 0.5
```

## Paramètres de commande

### Paramètres d'entrée

| Paramètre | Description | Valeur par défaut |
|-----------|-------------|-------------------|
| `--source_image` | Chemin de l'image portrait d'entrée | Requis |
| `--driving_video` | Chemin de la vidéo source | Optionnel |
| `--driven_audio` | Chemin de l'audio source | Optionnel |
| `--result_dir` | Répertoire de sortie | ./results |

### Paramètres de traitement

| Paramètre | Description | Valeur par défaut |
|-----------|-------------|-------------------|
| `--stitching` | Activer le stitching | False |
| `--eye_retargeting` | Re-targeting des yeux (0-1) | 0 |
| `--lip_retargeting` | Re-targeting des lèvres (0-1) | 0 |
| `--face_parser` | Activer l'analyse faciale | True |

### Paramètres de performance

| Paramètre | Description | Valeur par défaut |
|-----------|-------------|-------------------|
| `--batch_size` | Taille du batch | 1 |
| `--fps` | Fréquence d'images vidéo de sortie | 30 |

## Exemples d'utilisation

### Exemple 1 : Animation vidéo pilotée de base

```bash
python inference.py \
  --source_image examples/source_image.jpg \
  --driving_video examples/driving_video.mp4 \
  --result_dir results/basic
```

### Exemple 2 : Avec stitching

```bash
python inference.py \
  --source_image examples/source_image.jpg \
  --driving_video examples/driving_video.mp4 \
  --result_dir results/stitching \
  --stitching
```

### Exemple 3 : Contrôle des yeux

```bash
python inference.py \
  --source_image examples/source_image.jpg \
  --driving_video examples/driving_video.mp4 \
  --result_dir results/eyes \
  --eye_retargeting 0.8
```

### Exemple 4 : Contrôle de la synchronisation des lèvres

```bash
python inference.py \
  --source_image examples/source_image.jpg \
  --driving_video examples/driving_video.mp4 \
  --result_dir results/lips \
  --lip_retargeting 0.7
```

### Exemple 5 : Contrôle complet

```bash
python inference.py \
  --source_image examples/source_image.jpg \
  --driving_video examples/driving_video.mp4 \
  --result_dir results/full \
  --stitching \
  --eye_retargeting 0.5 \
  --lip_retargeting 0.5
```

## API Python

### Utilisation de base

```python
from liveportrait import LivePortrait

# Initialiser
lp = LivePortrait()

# Animer le portrait
result = lp.animate(
    source_image="image.jpg",
    driving_video="video.mp4",
    stitching=True,
    eye_retargeting=0.5,
    lip_retargeting=0.5
)

print(f"Vidéo enregistrée dans : {result}")
```

### Utilisation avancée

```python
from liveportrait import LivePortrait
import torch

# Initialiser avec paramètres personnalisés
lp = LivePortrait(
    checkpoint_path="pretrained_models/liveportrait.pth",
    device="cuda" if torch.cuda.is_available() else "cpu"
)

# Générer avec contrôle complet
result = lp.animate(
    source_image="image.jpg",
    driving_video="video.mp4",
    stitching=True,
    eye_retargeting=0.8,
    lip_retargeting=0.8,
    output_fps=30,
    output_resolution=(512, 512)
)
```

### Traitement par lots

```python
from liveportrait import LivePortrait
import os

lp = LivePortrait()

# Traiter plusieurs images
source_dir = "source_images/"
driving_video = "driving.mp4"

for image_file in os.listdir(source_dir):
    if image_file.endswith(('.jpg', '.png')):
        result = lp.animate(
            source_image=os.path.join(source_dir, image_file),
            driving_video=driving_video,
            stitching=True
        )
```

## Interface web

### Lancer la démo web

```bash
python app.py
```

Puis ouvrir http://localhost:7860 dans le navigateur

### Fonctionnalités de l'interface web

1. **Télécharger l'image source** - Télécharger l'image portrait
2. **Télécharger la vidéo source** - Sélectionner la vidéo source
3. **Bouton stitching** - Activer/désactiver le stitching
4. **Contrôle des yeux** - Ajuster l'intensité du mouvement des yeux
5. **Contrôle des lèvres** - Ajuster l'intensité de synchronisation des lèvres
6. **Générer** - Créer l'animation
7. **Télécharger** - Enregistrer le résultat

## Fonctionnalités

### 1. Stitching

Le module de stitching connecte la tête générée au corps de manière transparente :

```bash
--stitching
```

- Transition fluide entre la tête et le corps
- Fonctionne avec différents styles d'images (réaliste, peinture à l'huile, sculpture, 3D)

### 2. Re-targeting des yeux

Contrôle l'ouverture des yeux avec une valeur scalaire (0-1) :

```bash
--eye_retargeting 0.5
```

- 0 : Yeux fermés
- 1 : Yeux complètement ouverts
- 0.5 : Naturel

### 3. Re-targeting des lèvres

Contrôle l'intensité du mouvement des lèvres :

```bash
--lip_retargeting 0.5
```

- 0 : Mouvement minimal
- 1 : Mouvement maximal
- 0.5 : Naturel

## Exigences d'entrée

### Image source

- Format : JPG, PNG
- Résolution : 512x512 ou supérieur recommandé
- Visage : De face, net
- Arrière-plan : N'importe lequel

### Vidéo source

- Format : MP4, AVI
- Durée : 1-60 secondes
- Résolution : N'importe laquelle
- Visage : Expressions faciales nettes

## Sortie

### Format vidéo

- Format : MP4
- Codec : H.264
- Résolution : 512x512
- Fréquence d'images : 30

### Répertoire de sortie

```
results/
├── output.mp4          # Vidéo générée
└── (fichiers temporaires)
```

## Dépannage

### Mauvaise qualité d'animation

**Solutions :**
- Utiliser une image source de meilleure qualité
- S'assurer que la vidéo source a un visage net
- Ajuster les paramètres de re-targeting

### Visage non détecté

**Solutions :**
- Utiliser une image portrait plus nette
- S'assurer que le visage est visible
- Vérifier le format de l'image

### Traitement lent

**Solutions :**
- Réduire la résolution de sortie
- Désactiver le stitching si pas nécessaire
- Utiliser l'accélération GPU

### Mémoire GPU insuffisante

**Solutions :**
- Réduire la taille du batch à 1
- Utiliser une taille d'image plus petite
- Fermer les autres applications GPU

## Conseils de performance

### Traitement plus rapide

1. Désactiver le stitching si pas nécessaire
2. Utiliser une résolution de sortie plus basse
3. Réduire les fps

### Meilleure qualité

1. Activer le stitching
2. Utiliser une image source de haute qualité
3. Ajuster les paramètres de re-targeting

## Liens connexes

- [GitHub](https://github.com/KwaiVGI/LivePortrait)
- [Article](https://arxiv.org/abs/2407.03168)
- [Démo](https://liveportrait.github.io)
