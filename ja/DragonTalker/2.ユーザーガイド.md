# DragonTalker ユーザーガイド

DragonTalkerは、深度学習を使用して単一の画像とオーディオ入力からリアルな話し頭ビデオを生成します。

## クイックスタート

### 基本コマンド

```bash
python inference.py \
  --source_image examples/source_image.jpg \
  --driven_audio examples/driven_audio.wav \
  --result_dir ./results
```

### エンハンスメント付き

```bash
python inference.py \
  --source_image examples/source_image.jpg \
  --driven_audio examples/driven_audio.wav \
  --result_dir ./results \
  --preprocess full \
  --enhancer gfpgan
```

## コマンドパラメータ

### 入力パラメータ

| パラメータ | 説明 | デフォルト |
|-----------|------|------------|
| `--source_image` | 入力ポートレート画像パス | 必須 |
| `--driven_audio` | 入力オーディオファイル (WAV/MP3) | 必須 |
| `--result_dir` | 出力ディレクトリ | ./results |

### 処理パラメータ

| パラメータ | 説明 | デフォルト |
|-----------|------|------------|
| `--preprocess` | 画像前処理：crop, resize, full | full |
| `--size` | 画像サイズ：256, 512 | 256 |
| `--pose_style` | ポーズスタイル 0-45 | 0 |
| `--expression_scale` | 表情強度 0.5-1.5 | 1.0 |

### エンハンスメントパラメータ

| パラメータ | 説明 | デフォルト |
|-----------|------|------------|
| `--enhancer` | 顔エンハンサー：gfpgan, RestoreFormer, CodeFormer | None |

### パフォーマンスパラメータ

| パラメータ | 説明 | デフォルト |
|-----------|------|------------|
| `--batch_size` | バッチサイズ | 1 |
| `--fps` | 出力ビデオフレームレート | 25 |

## 使用例

### 例1：基本生成

```bash
python inference.py \
  --source_image examples/source_image.jpg \
  --driven_audio examples/driven_audio.wav \
  --result_dir results/basic
```

### 例2：GFPGANエンハンスメント

```bash
python inference.py \
  --source_image examples/source_image.jpg \
  --driven_audio examples/driven_audio.wav \
  --result_dir results/enhanced \
  --enhancer gfpgan
```

### 例3：カスタムポーズ

```bash
python inference.py \
  --source_image examples/source_image.jpg \
  --driven_audio examples/driven_audio.wav \
  --result_dir results/pose5 \
  --pose_style 5
```

### 例4：高解像度

```bash
python inference.py \
  --source_image examples/source_image.jpg \
  --driven_audio examples/driven_audio.wav \
  --result_dir results/hd \
  --size 512 \
  --enhancer gfpgan
```

## Python API

### 基本的な使用法

```python
from inference import DragonTalker

# DragonTalkerを初期化
dragon = DragonTalker()

# ビデオを生成
video_path = dragon.generate(
    source_image="image.jpg",
    driven_audio="audio.wav",
    preprocess="full",
    enhancer="gfpgan"
)

print(f"ビデオ保存先: {video_path}")
```

### 上級使用法

```python
from inference import DragonTalker
import torch

# カスタム設定で初期化
dragon = DragonTalker(
    checkpoint_path="checkpoints/DragonTalker.pth",
    config_path="config/DragonTalker.yaml",
    device="cuda" if torch.cuda.is_available() else "cpu"
)

# 高度なオプションでビデオを生成
video_path = dragon.generate(
    source_image="image.jpg",
    driven_audio="audio.wav",
    preprocess="full",
    pose_style=10,
    expression_scale=1.2,
    enhancer="gfpgan",
    batch_size=1,
    output_video="output.mp4"
)
```

### バッチ処理

```python
from inference import DragonTalker
import os

dragon = DragonTalker()

# 複数の画像を処理（同じオーディオ）
audio_file = "audio.wav"
image_dir = "images/"

for image_file in os.listdir(image_dir):
    if image_file.endswith(('.jpg', '.png')):
        output_path = f"results/{image_file}"
        dragon.generate(
            source_image=os.path.join(image_dir, image_file),
            driven_audio=audio_file,
            preprocess="full"
        )
```

## Webインターフェース

### Webデモを実行

```bash
python app.py
```

ブラウザでhttp://localhost:8888を開きます

### Webインターフェース機能

1. **画像アップロード** - ポートレート画像をドラッグ＆ドロップ
2. **オーディオアップロード** - オーディオファイルを選択（WAV/MP3）
3. **プレビュー** - ソース画像とオーディオをプレビュー
4. **パラメータ調整** - ポーズ、表情、エンハンサーを調整
5. **生成** - ワンクリックでビデオを生成
6. **ダウンロード** - 生成したビデオを保存

## 顔エンハンサー

### 利用可能なエンハンサー

| エンハンサー | 品質 | 速度 | VRAM |
|--------------|------|------|------|
| なし | 基本的 | 速い | 低 |
| gfpgan | 良い | 中程度 | 中程度 |
| RestoreFormer | 非常に良い | 遅い | 高 |
| CodeFormer | 非常に良い | 遅い | 高 |

### 推奨

- **低VRAM**: エンハンサーなしまたはgfpgan
- **バランス**: gfpgan
- **最高品質**: RestoreFormerまたはCodeFormer

## 前処理モード

| モード | 説明 | ユースケース |
|--------|------|-------------|
| crop | 顔を中心切り抜き | 高速処理 |
| resize | ターゲットサイズにリサイズ | シンプルな背景 |
| full | 完全な処理パイプライン | 最高品質 |

## 入力要件

### 画像要件

- 形式：JPG、PNG
- 解像度：512x512以上を推奨
- 顔：正面向き、遮蔽なし
- 背景：シンプルが望ましい

### オーディオ要件

- 形式：WAV、MP3
- 長さ：1-60秒
- サンプルレート：16000-48000 Hz
-  Speech：明瞭、最小限のノイズ

## 出力

### ビデオ形式

- 形式：MP4
- コーデック：H.264
- 解像度：256x256または512x512
- フレームレート：25
- ビットレート：2-5 Mbps

### 出力ディレクトリ

結果は指定された出力ディレクトリに保存されます：
```
results/
├── video.mp4          # 生成されたビデオ
└── (一時ファイル)
```

## トラブルシューティング

### ビデオが暗すぎる/明るすぎる

**原因：** トレーニングと入力の間の照明の不一致

**解決策：**
- expression_scaleパラメータを調整 (0.8-1.2)
- より高品質のソース画像を使用
- 別のエンハンサーを試す

### 唇的不同期

**原因：** オーディオ品質または整合の問題

**解決策：**
- 高品質のオーディオを使用
- オーディオが明瞭なSpeechを含んでいることを確認
- オーディオとビデオの同期を確認

### 顔が変形する

**原因：** 入力品質が低いまたは異常

**解決策：**
- より高解像度の画像を使用 (512)
- 別の前処理モードを試す
- 顔エンハンサーを使用
- 顔が明確に可視であることを確認

### 処理が遅い

**解決策：**
- `--preprocess crop`を使用
- `--size 256`を設定
- エンハンサーを無効化
- バッチサイズを削減

### GPUメモリ不足

**解決策：**
- batch_sizeを1に削減
- より小さい画像サイズを使用
- 他のGPUアプリケーションを閉じる

## パフォーマンスのヒント

### より高速な処理

1. `--preprocess crop`を使用
2. `--size 256`を設定
3. エンハンサーを無効化

### より良い品質

1. `--size 512`を使用
2. `--enhancer gfpgan`を有効化
3. `--preprocess full`を使用

## 関連リンク

- [GitHub](https://github.com/your-repo/DragonTalker)
- [HuggingFace](https://huggingface.co/spaces)
- [モデル](https://github.com/your-repo/DragonTalker/releases)
