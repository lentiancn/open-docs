# V-Express Guide d'utilisation

V-Express génère des vidéos de tête parlante sous le contrôle d'une image de référence, d'un audio et de séquences V-Kps.

## Démarrage rapide

### Commande de base

```bash
python inference.py \
  --reference_image_path "./test_samples/short_case/AOC/ref.jpg" \
  --audio_path "./test_samples/short_case/AOC/aud.mp3" \
  --kps_path "./test_samples/short_case/AOC/kps.pth" \
  --output_path "./output/result.mp4" \
  --retarget_strategy "no_retarget" \
  --num_inference_steps 25
```

## Paramètres de commande

### Paramètres d'entrée

| Paramètre | Description | Requis |
|-----------|-------------|--------|
| `--reference_image_path` | Chemin de l'image portrait de référence | Oui |
| `--audio_path` | Chemin du fichier audio d'entrée (MP3/WAV) | Oui |
| `--kps_path` | Chemin du fichier de séquence V-Kps | Non |
| `--output_path` | Chemin de la vidéo de sortie | Oui |

### Stratégies de recadrage

| Stratégie | Description |
|-----------|-------------|
| `no_retarget` | Image et vidéo de la même personne (meilleurs résultats) |
| `fix_face` | Image et audio quelconque (sync labiale seulement) |
| `offset_retarget` | Image d'une autre personne avec léger mouvement facial |
| `naive_retarget` | Image d'une autre personne avec recadrage complet |

### Paramètres de traitement

| Paramètre | Description | Par défaut |
|-----------|-------------|------------|
| `--retarget_strategy` | Stratégie de recadrage du visage | no_retarget |
| `--num_inference_steps` | Nombre d'étapes d'inférence | 25 |
| `--reference_attention_weight` | Poids de l'image de référence (0.9-1.0) | 1.0 |
| `--audio_attention_weight` | Poids de l'audio (1.0-3.0) | 1.0 |
| `--save_gpu_memory` | Activer le mode économie de mémoire | False |

### Plages de paramètres recommandées

- `reference_attention_weight` : 0.9-1.0
- `audio_attention_weight` : 1.0-3.0

## Exemples d'utilisation

### Scénario 1 : Même personne (meilleure qualité)

Lorsque vous avez une image de la personne A et une vidéo parlante de la personne A :

```bash
python inference.py \
  --reference_image_path "./test_samples/short_case/AOC/ref.jpg" \
  --audio_path "./test_samples/short_case/AOC/aud.mp3" \
  --kps_path "./test_samples/short_case/AOC/kps.pth" \
  --output_path "./output/talk_AOC_no_retarget.mp4" \
  --retarget_strategy "no_retarget" \
  --num_inference_steps 25
```

### Scénario 2 : Audio quelconque (sync labiale seulement)

Lorsque vous avez seulement une image et un audio parlé quelconque :

```bash
python inference.py \
  --reference_image_path "./test_samples/short_case/tys/ref.jpg" \
  --audio_path "./test_samples/short_case/tys/aud.mp3" \
  --output_path "./output/talk_tys_fix_face.mp4" \
  --retarget_strategy "fix_face" \
  --num_inference_steps 25
```

### Scénario 3 : Personne différente avec mouvement facial

Lorsque vous avez une image de la personne A et une vidéo parlante de la personne B :

```bash
python inference.py \
  --reference_image_path "./test_samples/short_case/tys/ref.jpg" \
  --audio_path "./test_samples/short_case/tys/aud.mp3" \
  --kps_path "./test_samples/short_case/tys/kps.pth" \
  --output_path "./output/talk_tys_offset_retarget.mp4" \
  --retarget_strategy "offset_retarget" \
  --num_inference_steps 25
```

### Scénario 4 : Poids d'attention personnalisés

Ajuster les poids pour différents effets :

```bash
python inference.py \
  --reference_image_path "./test_samples/short_case/10/ref.jpg" \
  --audio_path "./test_samples/short_case/10/aud.mp3" \
  --output_path "./output/talk_10_weighted.mp4" \
  --retarget_strategy "fix_face" \
  --reference_attention_weight 0.95 \
  --audio_attention_weight 3.0
```

### Scénario 5 : Audio long (mémoire optimisée)

Pour les fichiers audio plus longs (30+ secondes) :

```bash
python inference.py \
  --reference_image_path "./test_samples/short_case/AOC/ref.jpg" \
  --audio_path "./test_samples/short_case/AOC/long_audio.mp3" \
  --kps_path "./test_samples/short_case/AOC/AOC_raw_kps.pth" \
  --output_path "./output/long_video.mp4" \
  --retarget_strategy "no_retarget" \
  --num_inference_steps 25 \
  --reference_attention_weight 1.0 \
  --audio_attention_weight 1.0 \
  --save_gpu_memory
```

## Extraire la séquence V-Kps

Si vous avez une vidéo cible, extrayez la séquence V-Kps :

```bash
python scripts/extract_kps_sequence_and_audio.py \
  --video_path "./test_samples/short_case/AOC/gt.mp4" \
  --kps_sequence_save_path "./test_samples/short_case/AOC/kps.pth" \
  --audio_save_path "./test_samples/short_case/AOC/aud.mp3"
```

## Exigences d'entrée

### Image de référence

- Format : JPG, PNG
- Résolution : 512x512 ou plus (carré recommandé)
- Visage : De face, net, sans occlusion
- Fond : Simple préféré

### Audio

- Format : MP3, WAV
- Durée : 1-60 secondes (plus long avec --save_gpu_memory)
- Fréquence d'échantillonnage : 16000-48000 Hz
- Parole : Nette, bruit minimum

### Séquence V-Kps

- Format : PyTorch (.pth)
- Générée à partir de la vidéo cible avec le script fourni

## Sortie

### Format vidéo

- Format : MP4
- Codec : H.264
- Résolution : Identique à l'entrée ou par défaut 512x512
- Images par seconde : 24-30

## Dépannage

### Mauvaise qualité vidéo

**Solutions :**
- Utiliser une image de référence de meilleure qualité (512x512+)
- Augmenter les étapes d'inférence (25-30)
- Ajuster les poids d'attention

### Problèmes de sync labiale

**Solutions :**
- Augmenter `audio_attention_weight` (1.5-3.0)
- Utiliser un audio de meilleure qualité
- S'assurer que l'audio contient une parole claire

### Déformation du visage

**Solutions :**
- Utiliser une pose similaire dans la vidéo cible
- Essayer une autre stratégie de recadrage
- Utiliser no_retarget pour la même personne

### Mémoire GPU insuffisante

**Solutions :**
- Activer `--save_gpu_memory`
- Réduire les étapes d'inférence
- Utiliser un audio plus court

### Traitement trop lent

**Solutions :**
- Réduire les étapes d'inférence (15-20)
- Utiliser un modèle plus petit
- Désactiver les fonctionnalités inutilisées

## Bonnes pratiques

1. **Vidéos de la même personne** : Utilisez la stratégie `no_retarget` pour de meilleurs résultats
2. **Personne différente** : Choisissez une vidéo cible avec une pose similaire au visage de référence
3. **Qualité d'image** : Utilisez des images de visage carrées nettes de 512x512
4. **Qualité audio** : Utilisez un audio avec une parole claire
5. **Ajustement des poids** : Ajustez les poids d'attention pour différents effets

## Liens connexes

- [GitHub](https://github.com/tencent-ailab/V-Express)
- [HuggingFace](https://huggingface.co/tk93/V-Express)
- [Article](https://arxiv.org/abs/2406.02511)
- [Page du projet](https://tenvence.github.io/p/v-express/)
