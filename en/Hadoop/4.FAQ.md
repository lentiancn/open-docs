# Hadoop FAQ

> Frequently Asked Questions about Apache Hadoop

---

## General

### What is Hadoop?

Apache Hadoop is an open-source framework for distributed storage and processing of large data sets. It uses HDFS for storage and MapReduce for processing.

### What are the main components?

- **HDFS**: Hadoop Distributed File System for storage
- **MapReduce**: Programming model for processing data
- **YARN**: Resource manager for job scheduling
- **Hadoop Common**: Utilities and libraries

---

## Installation

### What are the system requirements?

| Resource | Minimum |
|----------|---------|
| CPU | 2 cores |
| RAM | 4 GB |
| Disk | 10 GB |
| OS | Linux, Windows (with WSL), macOS |

### Single node setup?

**Steps**:
1. Download Hadoop binary
2. Extract to desired location
3. Configure JAVA_HOME in hadoop-env.sh
4. Configure core-site.xml, hdfs-site.xml, mapred-site.xml, yarn-site.xml
5. Format namenode: `hdfs namenode -format`
6. Start services: `start-all.sh`

---

## Usage

### How to create a directory in HDFS?

```bash
hdfs dfs -mkdir /user/username/directory
```

### How to copy files to HDFS?

```bash
hdfs dfs -put localfile.txt /user/username/
```

### How to run a MapReduce job?

```bash
hadoop jar job.jar input output
```

---

## Troubleshooting

### NameNode not starting?

**Solution**:
1. Check JAVA_HOME configuration
2. Verify directory permissions
3. Check logs in $HADOOP_HOME/logs
4. Try formatting namenode again

### OutOfMemoryError?

**Solution**:
1. Increase memory allocation in mapred-site.xml
2. Adjust map/reduce heap sizes

### DataNode not starting?

**Solution**:
1. Check network connectivity
2. Verify filesystem permissions
3. Review DataNode logs

---

## Performance

### How to improve performance?

1. Increase replication factor appropriately
2. Use compression for data storage
3. Optimize MapReduce combiners
4. Tune YARN resources
5. Use appropriate file formats (Parquet, ORC)

---

## Ecosystem

### What tools work with Hadoop?

- **Hive**: SQL-like queries
- **Pig**: Scripting for data analysis
- **Spark**: In-memory processing
- **HBase**: NoSQL database
- **Sqoop**: Data transfer to/from databases
- **Flume**: Log data collection

---

## Resources

- Official Docs: https://hadoop.apache.org/docs/
- Apache Hadoop Wiki: https://cwiki.apache.org/confluence/display/HADOOP/
